/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0605 20:43:16.123349 3185798 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0605 20:43:16.124081 3185798 gpu_resources.cc:164] device: 3, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
=======================================================================
I0605 20:43:17.233029 3185798 tcp_utils.cc:111] Retry to connect to 127.0.1.1:46267 while the server is not yet listening.
I0605 20:43:20.233258 3185798 tcp_utils.cc:134] Successfully connected to 127.0.1.1:46267
I0605 20:43:20.320927 3185798 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0605 20:43:20.320959 3185798 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0605 20:43:21.683668 3185798 gpu_resources.cc:306] WARNING: device: 3. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 53, in <module>
    engine.train()
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 360, in train
    acc = self.eval(epoch_id)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/decorator.py", line 232, in fun
    return caller(func, *(extras + args), **kw)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/base/dygraph/base.py", line 400, in _decorate_function
    return func(*args, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 497, in eval
    eval_result = self.eval_func(self, epoch_id)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/evaluation/classification.py", line 62, in classification_eval
    out = engine.model(batch[0], depth=batch[1])
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/distributed/parallel.py", line 561, in forward
    outputs = self._layers(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/model_zoo/repvgg.py", line 346, in forward
    feat1 = self.cross_gsa_modules[1](feat1, rel_pos=geo1)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/base/theseus_layer.py", line 622, in forward
    qk_mat = qk_mat + mask
MemoryError: 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   add_ad_func(paddle::Tensor const&, paddle::Tensor const&)
1   paddle::experimental::add(paddle::Tensor const&, paddle::Tensor const&)
2   void phi::AddRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*)
3   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
4   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
5   paddle::memory::allocation::Allocator::Allocate(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 3. Cannot allocate 4.882812GB memory on GPU 3, 20.149719GB memory has been allocated and available memory is only 3.490479GB.

Please check whether there is any other process using GPU 3.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

I0605 20:44:08.079833 3185798 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0605 21:37:37.434226 3214894 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0605 21:37:37.434919 3214894 gpu_resources.cc:164] device: 3, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
=======================================================================
I0605 21:37:38.524735 3214894 tcp_utils.cc:111] Retry to connect to 127.0.1.1:53260 while the server is not yet listening.
I0605 21:37:41.525182 3214894 tcp_utils.cc:134] Successfully connected to 127.0.1.1:53260
I0605 21:37:41.551220 3214894 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0605 21:37:41.551254 3214894 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0605 21:37:42.968611 3214894 gpu_resources.cc:306] WARNING: device: 3. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 53, in <module>
    engine.train()
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 360, in train
    acc = self.eval(epoch_id)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/decorator.py", line 232, in fun
    return caller(func, *(extras + args), **kw)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/base/dygraph/base.py", line 400, in _decorate_function
    return func(*args, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 497, in eval
    eval_result = self.eval_func(self, epoch_id)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/evaluation/classification.py", line 62, in classification_eval
    out = engine.model(batch[0], depth=batch[1])
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/distributed/parallel.py", line 561, in forward
    outputs = self._layers(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/model_zoo/repvgg.py", line 346, in forward
    feat1 = self.cross_gsa_modules[1](feat1, rel_pos=geo1)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/base/theseus_layer.py", line 622, in forward
    qk_mat = qk_mat + mask
MemoryError: 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   add_ad_func(paddle::Tensor const&, paddle::Tensor const&)
1   paddle::experimental::add(paddle::Tensor const&, paddle::Tensor const&)
2   void phi::AddRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*)
3   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
4   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
5   paddle::memory::allocation::Allocator::Allocate(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 3. Cannot allocate 4.882812GB memory on GPU 3, 20.149719GB memory has been allocated and available memory is only 3.490479GB.

Please check whether there is any other process using GPU 3.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

I0605 21:38:28.953449 3214894 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0605 21:41:31.787005 3218901 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0605 21:41:31.787712 3218901 gpu_resources.cc:164] device: 3, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
=======================================================================
I0605 21:41:32.901458 3218901 tcp_utils.cc:111] Retry to connect to 127.0.1.1:38907 while the server is not yet listening.
I0605 21:41:35.901669 3218901 tcp_utils.cc:134] Successfully connected to 127.0.1.1:38907
I0605 21:41:35.947115 3218901 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0605 21:41:35.947150 3218901 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0605 21:41:37.435014 3218901 gpu_resources.cc:306] WARNING: device: 3. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 53, in <module>
    engine.train()
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 360, in train
    acc = self.eval(epoch_id)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/decorator.py", line 232, in fun
    return caller(func, *(extras + args), **kw)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/base/dygraph/base.py", line 400, in _decorate_function
    return func(*args, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 497, in eval
    eval_result = self.eval_func(self, epoch_id)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/evaluation/classification.py", line 62, in classification_eval
    out = engine.model(batch[0], depth=batch[1])
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/distributed/parallel.py", line 561, in forward
    outputs = self._layers(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/model_zoo/repvgg.py", line 346, in forward
    feat1 = self.cross_gsa_modules[1](feat1, rel_pos=geo1)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/base/theseus_layer.py", line 622, in forward
    qk_mat = qk_mat + mask
MemoryError: 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   add_ad_func(paddle::Tensor const&, paddle::Tensor const&)
1   paddle::experimental::add(paddle::Tensor const&, paddle::Tensor const&)
2   void phi::AddRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*)
3   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
4   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
5   paddle::memory::allocation::Allocator::Allocate(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 3. Cannot allocate 4.882812GB memory on GPU 3, 20.149719GB memory has been allocated and available memory is only 3.490479GB.

Please check whether there is any other process using GPU 3.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

I0605 21:42:28.519068 3218901 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0605 22:05:45.471781 3244344 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0605 22:05:45.472407 3244344 gpu_resources.cc:164] device: 3, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
=======================================================================
I0605 22:05:46.649897 3244344 tcp_utils.cc:134] Successfully connected to 127.0.1.1:55308
I0605 22:05:46.650172 3244344 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0605 22:05:46.650188 3244344 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0605 22:05:52.745333 3244344 gpu_resources.cc:306] WARNING: device: 3. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 55, in <module>
    engine.train()
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 344, in train
    self.train_epoch_func(self, epoch_id, print_batch_step)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 65, in train_epoch
    out = forward(engine, batch)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 117, in forward
    return engine.model(batch[0],depth=batch[1])
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/distributed/parallel.py", line 561, in forward
    outputs = self._layers(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/model_zoo/repvgg.py", line 346, in forward
    feat1 = self.cross_gsa_modules[1](feat1, rel_pos=geo1)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/base/theseus_layer.py", line 618, in forward
    qk_mat = paddle.matmul(qr, paddle.transpose(kr, perm=[0,1,3,2]))
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/tensor/linalg.py", line 321, in matmul
    return _C_ops.matmul(x, y, transpose_x, transpose_y)
MemoryError: 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_matmul(_object*, _object*, _object*)
1   matmul_ad_func(paddle::Tensor const&, paddle::Tensor const&, bool, bool)
2   paddle::experimental::matmul(paddle::Tensor const&, paddle::Tensor const&, bool, bool)
3   void phi::MatmulKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, bool, bool, phi::DenseTensor*)
4   void phi::MatMulFunctionImplWithBlas<phi::GPUContext, float>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, std::vector<long, std::allocator<long> > const&, std::vector<long, std::allocator<long> > const&, phi::DenseTensor*, bool, bool, bool, phi::funcs::MatmulPlanner*)
5   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
6   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 3. Cannot allocate 9.378906GB memory on GPU 3, 17.595032GB memory has been allocated and available memory is only 6.045166GB.

Please check whether there is any other process using GPU 3.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

I0605 22:06:03.497967 3244344 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0605 22:15:09.272593 3250911 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0605 22:15:09.273264 3250911 gpu_resources.cc:164] device: 3, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
=======================================================================
I0605 22:15:10.453737 3250911 tcp_utils.cc:134] Successfully connected to 127.0.1.1:48437
I0605 22:15:10.520092 3250911 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0605 22:15:10.520114 3250911 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0605 22:15:14.791426 3250911 gpu_resources.cc:306] WARNING: device: 3. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
I0606 01:00:00.609618 3250911 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0606 09:50:06.022075 4062925 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0606 09:50:06.022737 4062925 gpu_resources.cc:164] device: 3, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
=======================================================================
I0606 09:50:06.475925 4062925 tcp_utils.cc:134] Successfully connected to 127.0.1.1:35819
I0606 09:50:06.503211 4062925 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0606 09:50:06.503249 4062925 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0606 09:50:08.927759 4062925 gpu_resources.cc:306] WARNING: device: 3. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
I0606 09:52:14.202098 4062925 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0606 09:53:09.815872 4070678 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0606 09:53:09.816501 4070678 gpu_resources.cc:164] device: 3, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
=======================================================================
I0606 09:53:10.303318 4070678 tcp_utils.cc:111] Retry to connect to 127.0.1.1:57799 while the server is not yet listening.
I0606 09:53:13.303470 4070678 tcp_utils.cc:134] Successfully connected to 127.0.1.1:57799
I0606 09:53:13.333057 4070678 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0606 09:53:13.333072 4070678 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0606 09:53:15.661043 4070678 gpu_resources.cc:306] WARNING: device: 3. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
I0606 12:41:35.541462 4070678 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0606 20:16:15.518766 2970358 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0606 20:16:15.519433 2970358 gpu_resources.cc:164] device: 3, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
=======================================================================
I0606 20:16:16.484812 2970358 tcp_utils.cc:134] Successfully connected to 127.0.1.1:61030
I0606 20:16:16.552069 2970358 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0606 20:16:16.552093 2970358 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0606 20:16:21.788571 2970358 gpu_resources.cc:306] WARNING: device: 3. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Backward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   StackGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::stack_grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, paddle::Tensor const&, int, std::vector<paddle::Tensor*, std::allocator<paddle::Tensor*> >)
4   phi::KernelImpl<void (*)(phi::GPUContext const&, phi::DenseTensor const&, int, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::StackGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, int, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, phi::DenseTensor const&, int, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::funcs::UnStackRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, int, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >*)
6   void phi::funcs::LaunchUnStackKernel<phi::GPUContext, float, int, (phi::funcs::SegmentedArraySize)4>(phi::GPUContext const&, int, int, int, int, phi::DenseTensor const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >*)

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1749212194 (unix time) try "date -d @1749212194" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0x3e8002d510f) received by PID 2970358 (TID 0x7e697a01b740) from PID 2969871 ***]

/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0606 20:17:28.895388 2988222 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0606 20:17:28.896014 2988222 gpu_resources.cc:164] device: 3, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
=======================================================================
I0606 20:17:29.996086 2988222 tcp_utils.cc:111] Retry to connect to 127.0.1.1:58310 while the server is not yet listening.
I0606 20:17:32.996299 2988222 tcp_utils.cc:134] Successfully connected to 127.0.1.1:58310
I0606 20:17:33.038082 2988222 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0606 20:17:33.038106 2988222 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0606 20:17:35.266892 2988222 gpu_resources.cc:306] WARNING: device: 3. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
After downsampling, x_reshaped shape: [32, 96, 28, 28], H: 28, W: 28
After downsampling, x_reshaped shape: [32, 96, 28, 28], H: 28, W: 28
After downsampling, x_reshaped shape: [32, 192, 14, 14], H: 14, W: 14
After downsampling, x_reshaped shape: [32, 192, 14, 14], H: 14, W: 14
After downsampling, x_reshaped shape: [32, 192, 14, 14], H: 14, W: 14
After downsampling, x_reshaped shape: [32, 192, 14, 14], H: 14, W: 14
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 768, 3, 3], H: 3, W: 3
After downsampling, x_reshaped shape: [32, 768, 3, 3], H: 3, W: 3


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Backward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   StackGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::stack_grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, paddle::Tensor const&, int, std::vector<paddle::Tensor*, std::allocator<paddle::Tensor*> >)
4   phi::KernelImpl<void (*)(phi::GPUContext const&, phi::DenseTensor const&, int, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::StackGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, int, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, phi::DenseTensor const&, int, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::funcs::UnStackRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, int, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >*)
6   void phi::funcs::LaunchUnStackKernel<phi::GPUContext, float, int, (phi::funcs::SegmentedArraySize)4>(phi::GPUContext const&, int, int, int, int, phi::DenseTensor const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >*)

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1749212268 (unix time) try "date -d @1749212268" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0x3e8002d9709) received by PID 2988222 (TID 0x7477a1660740) from PID 2987785 ***]

/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0606 20:35:10.512523 3248853 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0606 20:35:10.513190 3248853 gpu_resources.cc:164] device: 3, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
=======================================================================
I0606 20:35:10.977248 3248853 tcp_utils.cc:111] Retry to connect to 127.0.1.1:61075 while the server is not yet listening.
I0606 20:35:13.977422 3248853 tcp_utils.cc:134] Successfully connected to 127.0.1.1:61075
I0606 20:35:13.977634 3248853 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0606 20:35:13.977645 3248853 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0606 20:35:16.424713 3248853 gpu_resources.cc:306] WARNING: device: 3. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
feat2.shape after GSA: [16, 56, 56, 160]


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Backward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   Conv2dGradNodeFinal::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::conv2d_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&, paddle::Tensor*, paddle::Tensor*)
4   void phi::ConvCudnnGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&, phi::DenseTensor*, phi::DenseTensor*)
5   std::_Function_handler<void (void*), phi::ConvRunner<float, (phi::ConvKind)2>::Apply(phi::GPUContext const&, phi::ConvArgsBase<cudnnContext*, cudnnDataType_t> const&, phi::SearchResult<cudnnConvolutionBwdDataAlgo_t> const&, float const*, float const*, float*, int, int, int, int, unsigned long, phi::DnnWorkspaceHandle*, bool)::{lambda(void*)#1}>::_M_invoke(std::_Any_data const&, void*&&)

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1749213328 (unix time) try "date -d @1749213328" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0x3e800319136) received by PID 3248853 (TID 0x72140d915740) from PID 3248438 ***]

/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0606 20:36:20.892613 3257164 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0606 20:36:20.893244 3257164 gpu_resources.cc:164] device: 3, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
=======================================================================
I0606 20:36:21.368093 3257164 tcp_utils.cc:134] Successfully connected to 127.0.1.1:37936
I0606 20:36:21.431133 3257164 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0606 20:36:21.431164 3257164 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0606 20:36:23.789327 3257164 gpu_resources.cc:306] WARNING: device: 3. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
W0606 20:45:54.176476 3282727 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0606 20:45:54.177204 3282727 gpu_resources.cc:164] device: 3, cuDNN Version: 8.9.
feat2.shape after GSA: [16, 56, 56, 160]
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
=======================================================================
I0606 20:45:54.709506 3282727 tcp_utils.cc:134] Successfully connected to 127.0.1.1:60980
I0606 20:45:54.784029 3282727 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0606 20:45:54.784051 3282727 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
W0606 20:46:04.678156 3282727 gpu_resources.cc:306] WARNING: device: 3. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_conv2d(_object*, _object*, _object*)
1   conv2d_ad_func(paddle::Tensor const&, paddle::Tensor const&, std::vector<int, std::allocator<int> >, std::vector<int, std::allocator<int> >, std::string, std::vector<int, std::allocator<int> >, int, std::string)
2   paddle::experimental::conv2d(paddle::Tensor const&, paddle::Tensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&)
3   void phi::ConvCudnnKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&, phi::DenseTensor*)
4   void phi::ConvCudnnKernelImplV7<float, phi::GPUContext>(phi::DenseTensor const*, phi::DenseTensor const*, phi::GPUContext const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, phi::backends::gpu::DataLayout, phi::backends::gpu::DataLayout, bool, bool, int, phi::DenseTensor*)
5   phi::DnnWorkspaceHandle::ReallocWorkspace(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1749213972 (unix time) try "date -d @1749213972" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0x3e8003216c8) received by PID 3282727 (TID 0x71537d976740) from PID 3282632 ***]

feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
W0606 20:47:25.053582 3287209 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0606 20:47:25.054596 3287209 gpu_resources.cc:164] device: 3, cuDNN Version: 8.9.
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
=======================================================================
I0606 20:47:25.573869 3287209 tcp_utils.cc:134] Successfully connected to 127.0.1.1:60650
I0606 20:47:25.630030 3287209 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0606 20:47:25.630044 3287209 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
W0606 20:47:29.966747 3287209 gpu_resources.cc:306] WARNING: device: 3. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 55, in <module>
    engine.train()
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 344, in train
    self.train_epoch_func(self, epoch_id, print_batch_step)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 65, in train_epoch
    out = forward(engine, batch)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 117, in forward
    return engine.model(batch[0],depth=batch[1])
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/distributed/parallel.py", line 561, in forward
    outputs = self._layers(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/model_zoo/repvgg.py", line 329, in forward
    out1 = self.stage1(out)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/container.py", line 754, in forward
    input = layer(input)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/model_zoo/repvgg.py", line 175, in forward
    self.se(self.rbr_dense(inputs) + self.rbr_1x1(inputs) + id_out))
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/model_zoo/repvgg.py", line 81, in forward
    y = self.conv(x)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/conv.py", line 742, in forward
    out = F.conv._conv_nd(
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/functional/conv.py", line 150, in _conv_nd
    pre_bias = _C_ops.conv2d(
MemoryError: 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_conv2d(_object*, _object*, _object*)
1   conv2d_ad_func(paddle::Tensor const&, paddle::Tensor const&, std::vector<int, std::allocator<int> >, std::vector<int, std::allocator<int> >, std::string, std::vector<int, std::allocator<int> >, int, std::string)
2   paddle::experimental::conv2d(paddle::Tensor const&, paddle::Tensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&)
3   void phi::ConvCudnnKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&, phi::DenseTensor*)
4   void phi::ConvCudnnKernelImplV7<float, phi::GPUContext>(phi::DenseTensor const*, phi::DenseTensor const*, phi::GPUContext const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, phi::backends::gpu::DataLayout, phi::backends::gpu::DataLayout, bool, bool, int, phi::DenseTensor*)
5   phi::DnnWorkspaceHandle::ReallocWorkspace(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 3. Cannot allocate 141.328125MB memory on GPU 3, 23.528198GB memory has been allocated and available memory is only 114.687500MB.

Please check whether there is any other process using GPU 3.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

feat2.shape after GSA: [16, 56, 56, 160]
I0606 20:47:40.655620 3287209 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0606 20:48:29.186054 3290701 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0606 20:48:29.186654 3290701 gpu_resources.cc:164] device: 3, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
=======================================================================
I0606 20:48:29.600452 3290701 tcp_utils.cc:134] Successfully connected to 127.0.1.1:57553
I0606 20:48:29.639036 3290701 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0606 20:48:29.639060 3290701 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
W0606 20:48:36.416874 3290701 gpu_resources.cc:306] WARNING: device: 3. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_conv2d(_object*, _object*, _object*)
1   conv2d_ad_func(paddle::Tensor const&, paddle::Tensor const&, std::vector<int, std::allocator<int> >, std::vector<int, std::allocator<int> >, std::string, std::vector<int, std::allocator<int> >, int, std::string)
2   paddle::experimental::conv2d(paddle::Tensor const&, paddle::Tensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&)
3   void phi::ConvCudnnKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&, phi::DenseTensor*)
4   void phi::ConvCudnnKernelImplV7<float, phi::GPUContext>(phi::DenseTensor const*, phi::DenseTensor const*, phi::GPUContext const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, phi::backends::gpu::DataLayout, phi::backends::gpu::DataLayout, bool, bool, int, phi::DenseTensor*)
5   phi::DnnWorkspaceHandle::ReallocWorkspace(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1749214124 (unix time) try "date -d @1749214124" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0x3e8003235d7) received by PID 3290701 (TID 0x79424263e740) from PID 3290583 ***]

feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
W0606 20:49:55.424346 3294636 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0606 20:49:55.425189 3294636 gpu_resources.cc:164] device: 3, cuDNN Version: 8.9.
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
=======================================================================
I0606 20:49:55.996667 3294636 tcp_utils.cc:134] Successfully connected to 127.0.1.1:53732
I0606 20:49:56.068070 3294636 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0606 20:49:56.068107 3294636 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 55, in <module>
    engine.train()
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 331, in train
    metric_info = init_model(self.config.Global, self.model,
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/utils/save_load.py", line 130, in init_model
    assert os.path.exists(checkpoints + ".pdparams"), \
AssertionError: Given dir https://paddle-imagenet-models-name.bj.bcebos.com/dygraph/RepVGG_D2se_pretrained.pdparams.pdparams not exist.
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
I0606 20:50:00.051658 3294636 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
W0606 20:50:33.764117 3296519 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0606 20:50:33.764917 3296519 gpu_resources.cc:164] device: 3, cuDNN Version: 8.9.
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
=======================================================================
I0606 20:50:34.309001 3296519 tcp_utils.cc:134] Successfully connected to 127.0.1.1:35733
I0606 20:50:34.387043 3296519 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0606 20:50:34.387070 3296519 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 55, in <module>
    engine.train()
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 331, in train
    metric_info = init_model(self.config.Global, self.model,
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/utils/save_load.py", line 130, in init_model
    assert os.path.exists(checkpoints + ".pdparams"), \
AssertionError: Given dir https://paddle-imagenet-models-name.bj.bcebos.com/dygraph/RepVGG_D2se_pretrained.pdparams.pdparams not exist.
I0606 20:50:38.417384 3296519 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
W0606 20:51:56.725718 3300090 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0606 20:51:56.726469 3300090 gpu_resources.cc:164] device: 3, cuDNN Version: 8.9.
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
=======================================================================
I0606 20:52:01.039431 3300090 tcp_utils.cc:134] Successfully connected to 127.0.1.1:54280
I0606 20:52:01.045020 3300090 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0606 20:52:01.045034 3300090 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
W0606 20:52:05.459291 3300090 gpu_resources.cc:306] WARNING: device: 3. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 55, in <module>
    engine.train()
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 344, in train
    self.train_epoch_func(self, epoch_id, print_batch_step)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 65, in train_epoch
    out = forward(engine, batch)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 117, in forward
    return engine.model(batch[0],depth=batch[1])
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/distributed/parallel.py", line 561, in forward
    outputs = self._layers(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/model_zoo/repvgg.py", line 329, in forward
    out1 = self.stage1(out)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/container.py", line 754, in forward
    input = layer(input)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/model_zoo/repvgg.py", line 175, in forward
    self.se(self.rbr_dense(inputs) + self.rbr_1x1(inputs) + id_out))
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/model_zoo/repvgg.py", line 81, in forward
    y = self.conv(x)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/conv.py", line 742, in forward
    out = F.conv._conv_nd(
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/functional/conv.py", line 150, in _conv_nd
    pre_bias = _C_ops.conv2d(
MemoryError: 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_conv2d(_object*, _object*, _object*)
1   conv2d_ad_func(paddle::Tensor const&, paddle::Tensor const&, std::vector<int, std::allocator<int> >, std::vector<int, std::allocator<int> >, std::string, std::vector<int, std::allocator<int> >, int, std::string)
2   paddle::experimental::conv2d(paddle::Tensor const&, paddle::Tensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&)
3   void phi::ConvCudnnKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&, phi::DenseTensor*)
4   void phi::ConvCudnnKernelImplV7<float, phi::GPUContext>(phi::DenseTensor const*, phi::DenseTensor const*, phi::GPUContext const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, phi::backends::gpu::DataLayout, phi::backends::gpu::DataLayout, bool, bool, int, phi::DenseTensor*)
5   phi::DnnWorkspaceHandle::ReallocWorkspace(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 3. Cannot allocate 141.328125MB memory on GPU 3, 23.536011GB memory has been allocated and available memory is only 106.687500MB.

Please check whether there is any other process using GPU 3.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

feat2.shape after GSA: [16, 56, 56, 160]
I0606 20:52:16.285750 3300090 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0606 20:54:22.897444 3307155 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0606 20:54:22.898062 3307155 gpu_resources.cc:164] device: 3, cuDNN Version: 8.9.
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
=======================================================================
I0606 20:54:27.102267 3307155 tcp_utils.cc:134] Successfully connected to 127.0.1.1:47225
I0606 20:54:27.102406 3307155 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0606 20:54:27.102416 3307155 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
W0606 20:54:29.005055 3307155 gpu_resources.cc:306] WARNING: device: 3. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 55, in <module>
    engine.train()
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 344, in train
    self.train_epoch_func(self, epoch_id, print_batch_step)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 65, in train_epoch
    out = forward(engine, batch)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 117, in forward
    return engine.model(batch[0],depth=batch[1])
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/distributed/parallel.py", line 561, in forward
    outputs = self._layers(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/model_zoo/repvgg.py", line 329, in forward
    out1 = self.stage1(out)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/container.py", line 754, in forward
    input = layer(input)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/model_zoo/repvgg.py", line 175, in forward
    self.se(self.rbr_dense(inputs) + self.rbr_1x1(inputs) + id_out))
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/model_zoo/repvgg.py", line 81, in forward
    y = self.conv(x)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/conv.py", line 742, in forward
    out = F.conv._conv_nd(
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/functional/conv.py", line 150, in _conv_nd
    pre_bias = _C_ops.conv2d(
MemoryError: 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_conv2d(_object*, _object*, _object*)
1   conv2d_ad_func(paddle::Tensor const&, paddle::Tensor const&, std::vector<int, std::allocator<int> >, std::vector<int, std::allocator<int> >, std::string, std::vector<int, std::allocator<int> >, int, std::string)
2   paddle::experimental::conv2d(paddle::Tensor const&, paddle::Tensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&)
3   void phi::ConvCudnnKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&, phi::DenseTensor*)
4   void phi::ConvCudnnKernelImplV7<float, phi::GPUContext>(phi::DenseTensor const*, phi::DenseTensor const*, phi::GPUContext const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, phi::backends::gpu::DataLayout, phi::backends::gpu::DataLayout, bool, bool, int, phi::DenseTensor*)
5   phi::DnnWorkspaceHandle::ReallocWorkspace(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 3. Cannot allocate 141.328125MB memory on GPU 3, 23.536011GB memory has been allocated and available memory is only 106.687500MB.

Please check whether there is any other process using GPU 3.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

I0606 20:54:39.725834 3307155 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0606 20:59:23.826823 3322159 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0606 20:59:23.827486 3322159 gpu_resources.cc:164] device: 3, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
=======================================================================
I0606 20:59:27.862178 3322159 tcp_utils.cc:134] Successfully connected to 127.0.1.1:60730
I0606 20:59:27.892026 3322159 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0606 20:59:27.892050 3322159 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
feat2.shape after GSA: [16, 56, 56, 160]
W0606 20:59:29.706265 3322159 gpu_resources.cc:306] WARNING: device: 3. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 55, in <module>
    engine.train()
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 344, in train
    self.train_epoch_func(self, epoch_id, print_batch_step)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 65, in train_epoch
    out = forward(engine, batch)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 117, in forward
    return engine.model(batch[0],depth=batch[1])
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/distributed/parallel.py", line 561, in forward
    outputs = self._layers(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/model_zoo/repvgg.py", line 339, in forward
    geo1 = self.geo_prior_gen((H1, W1), depth)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/base/theseus_layer.py", line 504, in forward
    mask = self.weight[0] * mask + self.weight[1] * mask_d
MemoryError: 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   add_ad_func(paddle::Tensor const&, paddle::Tensor const&)
1   paddle::experimental::add(paddle::Tensor const&, paddle::Tensor const&)
2   void phi::AddRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*)
3   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
4   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
5   paddle::memory::allocation::Allocator::Allocate(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 3. Cannot allocate 1.172363GB memory on GPU 3, 23.506714GB memory has been allocated and available memory is only 136.687500MB.

Please check whether there is any other process using GPU 3.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

feat2.shape after GSA: [16, 56, 56, 160]
I0606 20:59:40.555974 3322159 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0606 21:00:05.165462 3324328 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0606 21:00:05.166126 3324328 gpu_resources.cc:164] device: 3, cuDNN Version: 8.9.
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
=======================================================================
I0606 21:00:09.313920 3324328 tcp_utils.cc:134] Successfully connected to 127.0.1.1:61053
I0606 21:00:09.362167 3324328 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0606 21:00:09.362218 3324328 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
W0606 21:00:11.057550 3324328 gpu_resources.cc:306] WARNING: device: 3. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
W0606 21:00:15.353329 3324996 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0606 21:00:15.353952 3324996 gpu_resources.cc:164] device: 3, cuDNN Version: 8.9.
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 32, in <module>
    paddle.device.cuda.empty_cache()
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/device/cuda/__init__.py", line 189, in empty_cache
    core.cuda_empty_cache()
OSError: (External) CUDA error(2), out of memory. 
  [Hint: 'cudaErrorMemoryAllocation'. The API call failed because it was unable to allocate enough memory to perform the requested operation. ] (at ../paddle/phi/backends/gpu/cuda/cuda_info.cc:206)

feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 55, in <module>
    engine.train()
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 344, in train
    self.train_epoch_func(self, epoch_id, print_batch_step)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 65, in train_epoch
    out = forward(engine, batch)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 117, in forward
    return engine.model(batch[0],depth=batch[1])
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/distributed/parallel.py", line 561, in forward
    outputs = self._layers(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/model_zoo/repvgg.py", line 339, in forward
    geo1 = self.geo_prior_gen((H1, W1), depth)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/base/theseus_layer.py", line 504, in forward
    mask = self.weight[0] * mask + self.weight[1] * mask_d
MemoryError: 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   add_ad_func(paddle::Tensor const&, paddle::Tensor const&)
1   paddle::experimental::add(paddle::Tensor const&, paddle::Tensor const&)
2   void phi::AddRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*)
3   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
4   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
5   paddle::memory::allocation::Allocator::Allocate(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 3. Cannot allocate 1.172363GB memory on GPU 3, 23.506714GB memory has been allocated and available memory is only 136.687500MB.

Please check whether there is any other process using GPU 3.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

I0606 21:00:21.743505 3324328 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
I0606 21:30:24.952528 3257164 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0607 11:01:32.905282 3800431 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0607 11:01:32.906062 3800431 gpu_resources.cc:164] device: 3, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
=======================================================================
I0607 11:01:33.373945 3800431 tcp_utils.cc:134] Successfully connected to 127.0.1.1:57706
I0607 11:01:33.450141 3800431 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0607 11:01:33.450176 3800431 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0607 11:01:35.684350 3800431 gpu_resources.cc:306] WARNING: device: 3. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0607 11:18:11.402588 3837233 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0607 11:18:11.403584 3837233 gpu_resources.cc:164] device: 3, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
=======================================================================
I0607 11:18:11.877789 3837233 tcp_utils.cc:134] Successfully connected to 127.0.1.1:47528
I0607 11:18:11.888005 3837233 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0607 11:18:11.888021 3837233 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   phi::backends::gpu::GpuMemcpySync(void*, void const*, unsigned long, cudaMemcpyKind)

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1749266299 (unix time) try "date -d @1749266299" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0x3e8003a8ce7) received by PID 3837233 (TID 0x796705285740) from PID 3837159 ***]

/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0607 11:21:53.728766 3842528 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0607 11:21:53.729403 3842528 gpu_resources.cc:164] device: 3, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
=======================================================================
I0607 11:21:54.276268 3842528 tcp_utils.cc:134] Successfully connected to 127.0.1.1:35213
I0607 11:21:54.318099 3842528 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0607 11:21:54.318135 3842528 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0607 11:21:59.878369 3842528 gpu_resources.cc:306] WARNING: device: 3. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
out1.shape before EMAA: [16, 160, 56, 56]
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 55, in <module>
    engine.train()
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 344, in train
    self.train_epoch_func(self, epoch_id, print_batch_step)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 65, in train_epoch
    out = forward(engine, batch)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 117, in forward
    return engine.model(batch[0],depth=batch[1])
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/distributed/parallel.py", line 561, in forward
    outputs = self._layers(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/model_zoo/repvgg.py", line 354, in forward
    out1 = self.EMAA(out1)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/base/theseus_layer.py", line 684, in forward
    hw = self.conv1x1(paddle.concat([x_h, x_w], axis=2))  # [b*g, c//g, h+w, 1]
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/conv.py", line 742, in forward
    out = F.conv._conv_nd(
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/functional/conv.py", line 150, in _conv_nd
    pre_bias = _C_ops.conv2d(
ValueError: (InvalidArgument) The number of input's channels should be equal to filter's channels * groups for Op(Conv). But received: the input's channels is 160, the input's shape is [16, 160, 112, 1]; the filter's channels is 40, the filter's shape is [40, 40, 1, 1]; the groups is 1, the data_format is NCHW. The error may come from wrong data_format setting.
  [Hint: Expected input_channels == filter_channels * groups, but received input_channels:160 != filter_channels * groups:40.] (at ../paddle/phi/infermeta/binary.cc:651)

I0607 11:22:00.582039 3842528 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0607 11:23:22.800697 3844903 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0607 11:23:22.801433 3844903 gpu_resources.cc:164] device: 3, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
=======================================================================
I0607 11:23:23.253188 3844903 tcp_utils.cc:111] Retry to connect to 127.0.1.1:42966 while the server is not yet listening.
I0607 11:23:26.253348 3844903 tcp_utils.cc:134] Successfully connected to 127.0.1.1:42966
I0607 11:23:26.314013 3844903 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0607 11:23:26.314028 3844903 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0607 11:23:32.730906 3844903 gpu_resources.cc:306] WARNING: device: 3. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 55, in <module>
    engine.train()
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 360, in train
    acc = self.eval(epoch_id)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/decorator.py", line 232, in fun
    return caller(func, *(extras + args), **kw)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/base/dygraph/base.py", line 400, in _decorate_function
    return func(*args, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 497, in eval
    eval_result = self.eval_func(self, epoch_id)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/evaluation/classification.py", line 62, in classification_eval
    out = engine.model(batch[0], depth=batch[1])
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/distributed/parallel.py", line 561, in forward
    outputs = self._layers(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/model_zoo/repvgg.py", line 350, in forward
    feat1 = self.cross_gsa_modules[1](feat1, rel_pos=geo1)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/base/theseus_layer.py", line 624, in forward
    qk_mat = qk_mat + mask
MemoryError: 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   add_ad_func(paddle::Tensor const&, paddle::Tensor const&)
1   paddle::experimental::add(paddle::Tensor const&, paddle::Tensor const&)
2   void phi::AddRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*)
3   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
4   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
5   paddle::memory::allocation::Allocator::Allocate(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 3. Cannot allocate 2.441406GB memory on GPU 3, 21.391357GB memory has been allocated and available memory is only 2.248840GB.

Please check whether there is any other process using GPU 3.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

I0607 11:24:39.909066 3844903 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
I0607 11:36:46.661871 3800431 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0607 16:32:59.596314 4002661 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0607 16:32:59.596946 4002661 gpu_resources.cc:164] device: 3, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
=======================================================================
I0607 16:33:00.082412 4002661 tcp_utils.cc:134] Successfully connected to 127.0.1.1:63984
I0607 16:33:00.150105 4002661 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0607 16:33:00.150130 4002661 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0607 16:33:05.386469 4002661 gpu_resources.cc:306] WARNING: device: 3. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0607 16:45:57.692867 4047391 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0607 16:45:57.693583 4047391 gpu_resources.cc:164] device: 3, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
=======================================================================
I0607 16:45:59.104532 4047391 tcp_utils.cc:134] Successfully connected to 127.0.1.1:53137
I0607 16:45:59.136152 4047391 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0607 16:45:59.136178 4047391 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0607 16:46:00.940438 4047391 gpu_resources.cc:306] WARNING: device: 3. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 55, in <module>
    engine.train()
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 344, in train
    self.train_epoch_func(self, epoch_id, print_batch_step)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 65, in train_epoch
    out = forward(engine, batch)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 117, in forward
    return engine.model(batch[0],depth=batch[1])
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/distributed/parallel.py", line 561, in forward
    outputs = self._layers(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/model_zoo/repvgg.py", line 334, in forward
    out1 = self.stage1(out)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/container.py", line 754, in forward
    input = layer(input)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/model_zoo/repvgg.py", line 176, in forward
    self.se(self.rbr_dense(inputs) + self.rbr_1x1(inputs) + id_out))
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/model_zoo/repvgg.py", line 82, in forward
    y = self.conv(x)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/conv.py", line 742, in forward
    out = F.conv._conv_nd(
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/functional/conv.py", line 150, in _conv_nd
    pre_bias = _C_ops.conv2d(
MemoryError: 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_conv2d(_object*, _object*, _object*)
1   conv2d_ad_func(paddle::Tensor const&, paddle::Tensor const&, std::vector<int, std::allocator<int> >, std::vector<int, std::allocator<int> >, std::string, std::vector<int, std::allocator<int> >, int, std::string)
2   paddle::experimental::conv2d(paddle::Tensor const&, paddle::Tensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&)
3   void phi::ConvCudnnKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&, phi::DenseTensor*)
4   void phi::ConvCudnnKernelImplV7<float, phi::GPUContext>(phi::DenseTensor const*, phi::DenseTensor const*, phi::GPUContext const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, phi::backends::gpu::DataLayout, phi::backends::gpu::DataLayout, bool, bool, int, phi::DenseTensor*)
5   phi::DnnWorkspaceHandle::ReallocWorkspace(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 3. Cannot allocate 141.328125MB memory on GPU 3, 23.502625GB memory has been allocated and available memory is only 140.875000MB.

Please check whether there is any other process using GPU 3.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

I0607 16:46:11.757221 4047391 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0607 16:53:57.715489 4057566 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0607 16:53:57.716095 4057566 gpu_resources.cc:164] device: 3, cuDNN Version: 8.9.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1749286439 (unix time) try "date -d @1749286439" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0x3e8003de98b) received by PID 4057566 (TID 0x7e890a9d3740) from PID 4057483 ***]

/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0607 16:55:19.958773 4058408 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0607 16:55:19.959426 4058408 gpu_resources.cc:164] device: 3, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
=======================================================================
I0607 16:55:21.121311 4058408 tcp_utils.cc:134] Successfully connected to 127.0.1.1:43304
I0607 16:55:21.136121 4058408 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0607 16:55:21.136144 4058408 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0607 16:55:22.563306 4058408 gpu_resources.cc:306] WARNING: device: 3. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   void phi::funcs::set_constant_with_place<phi::GPUPlace>(phi::DeviceContext const&, phi::DenseTensor*, float)
1   phi::funcs::SetConstant<phi::GPUContext, float>::operator()(phi::GPUContext const&, phi::DenseTensor*, float)

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1749290350 (unix time) try "date -d @1749290350" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0x3e800011ebc) received by PID 4058408 (TID 0x70c8a4524740) from PID 73404 ***]

/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0607 18:00:21.761699 74147 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0607 18:00:21.762327 74147 gpu_resources.cc:164] device: 3, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
=======================================================================
I0607 18:00:22.944907 74147 tcp_utils.cc:111] Retry to connect to 127.0.1.1:44953 while the server is not yet listening.
I0607 18:00:25.945071 74147 tcp_utils.cc:134] Successfully connected to 127.0.1.1:44953
I0607 18:00:25.982218 74147 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0607 18:00:25.982239 74147 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0607 18:00:27.388012 74147 gpu_resources.cc:306] WARNING: device: 3. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   void phi::funcs::set_constant_with_place<phi::GPUPlace>(phi::DeviceContext const&, phi::DenseTensor*, float)
1   phi::funcs::SetConstant<phi::GPUContext, float>::operator()(phi::GPUContext const&, phi::DenseTensor*, float)

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1749293469 (unix time) try "date -d @1749293469" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0x3e80003dfba) received by PID 74147 (TID 0x71293d5e7740) from PID 253882 ***]

/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0607 18:56:40.727777 256563 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0607 18:56:40.728493 256563 gpu_resources.cc:164] device: 3, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
=======================================================================
I0607 18:56:41.927332 256563 tcp_utils.cc:134] Successfully connected to 127.0.1.1:57399
I0607 18:56:41.978127 256563 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0607 18:56:41.978199 256563 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0607 18:56:43.380664 256563 gpu_resources.cc:306] WARNING: device: 3. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
I0607 23:59:32.555158 256563 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0613 18:33:34.811365 248556 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0613 18:33:34.813869 248556 gpu_resources.cc:164] device: 3, cuDNN Version: 8.9.
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 54, in <module>
    engine = Engine(config, mode="train")
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 139, in __init__
    self.train_dataloader = build_dataloader(
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/data/__init__.py", line 116, in build_dataloader
    dataset = eval(dataset_name)(**config_dataset)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/data/dataloader/imagenet_dataset.py", line 45, in __init__
    super(ImageNetDataset, self).__init__(image_root, cls_label_path,
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/data/dataloader/common_dataset.py", line 63, in __init__
    self._load_anno()
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/data/dataloader/imagenet_dataset.py", line 85, in _load_anno
    assert os.path.exists(depth_path), f"depth path {depth_path} does not exist."
AssertionError: depth path /mnt/data1_hdd/wgk/PaddleClas/tr/datasetsmuti/train/4/(8)_disp_disp.png does not exist.
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0613 18:41:26.870779 253905 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0613 18:41:26.871470 253905 gpu_resources.cc:164] device: 3, cuDNN Version: 8.9.
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 54, in <module>
    engine = Engine(config, mode="train")
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 139, in __init__
    self.train_dataloader = build_dataloader(
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/data/__init__.py", line 116, in build_dataloader
    dataset = eval(dataset_name)(**config_dataset)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/data/dataloader/imagenet_dataset.py", line 45, in __init__
    super(ImageNetDataset, self).__init__(image_root, cls_label_path,
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/data/dataloader/common_dataset.py", line 63, in __init__
    self._load_anno()
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/data/dataloader/imagenet_dataset.py", line 79, in _load_anno
    assert os.path.exists(self.images[
AssertionError: path /mnt/data1_hdd/wgk/PaddleClas/tr/datasetsmuti/train/4/(7).jpg does not exist.
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0613 18:50:24.936328 261833 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0613 18:50:24.936970 261833 gpu_resources.cc:164] device: 3, cuDNN Version: 8.9.
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 54, in <module>
    engine = Engine(config, mode="train")
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 213, in __init__
    self.train_metric_func = build_metrics(metric_config)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/metric/__init__.py", line 75, in build_metrics
    metrics_list = CombinedMetrics(copy.deepcopy(config))
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/metric/__init__.py", line 46, in __init__
    eval(metric_name)(**metric_params))
  File "<string>", line 1, in <module>
NameError: name 'Auc' is not defined
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0613 18:53:03.166710 263687 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0613 18:53:03.167379 263687 gpu_resources.cc:164] device: 3, cuDNN Version: 8.9.
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 54, in <module>
    engine = Engine(config, mode="train")
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 213, in __init__
    self.train_metric_func = build_metrics(metric_config)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/metric/__init__.py", line 75, in build_metrics
    metrics_list = CombinedMetrics(copy.deepcopy(config))
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/metric/__init__.py", line 46, in __init__
    eval(metric_name)(**metric_params))
TypeError: __init__() got an unexpected keyword argument 'num_classes'
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0613 18:58:14.240608 267209 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0613 18:58:14.241246 267209 gpu_resources.cc:164] device: 3, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
=======================================================================
I0613 18:58:15.562121 267209 tcp_utils.cc:111] Retry to connect to 127.0.1.1:45901 while the server is not yet listening.
I0613 18:58:18.562331 267209 tcp_utils.cc:134] Successfully connected to 127.0.1.1:45901
I0613 18:58:18.604121 267209 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0613 18:58:18.604163 267209 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0613 18:58:20.430383 267209 gpu_resources.cc:306] WARNING: device: 3. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Backward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   Conv2dGradNodeFinal::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::conv2d_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&, paddle::Tensor*, paddle::Tensor*)
4   void phi::ConvCudnnGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&, phi::DenseTensor*, phi::DenseTensor*)
5   phi::SearchResult<cudnnConvolutionBwdDataAlgo_t> phi::SearchAlgorithm<(phi::ConvKind)2>::Find<float>(phi::GPUContext const&, phi::ConvArgsBase<cudnnContext*, cudnnDataType_t> const&, bool, bool, bool)

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1749812312 (unix time) try "date -d @1749812312" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0x3e800041386) received by PID 267209 (TID 0x724edf3f7740) from PID 267142 ***]

/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0613 19:11:33.156873 277257 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0613 19:11:33.157473 277257 gpu_resources.cc:164] device: 3, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cudnn_batchnorm_spatial_persistent', current_value=True, default_value=False)
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_max_inplace_grad_add', current_value=8, default_value=0)
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
=======================================================================
I0613 19:11:33.443626 277257 tcp_utils.cc:134] Successfully connected to 127.0.1.1:55550
I0613 19:11:33.510056 277257 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0613 19:11:33.510092 277257 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
Process Process-8:
Traceback (most recent call last):
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 401, in _worker_loop
    tensor_list = [
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 403, in <listcomp>
    numpy2lodtensor(b)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 398, in numpy2lodtensor
    lodtensor.set(arr, core.CPUPlace())
ValueError: (InvalidArgument) Input object type error or incompatible array data type. tensor.set() supports array with bool, float16, float32, float64, int8, int16, int32, int64, uint8 or uint16, please check your input or input array data type. (at ../paddle/fluid/pybind/tensor_py.h:565)

Process Process-6:
Traceback (most recent call last):
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 401, in _worker_loop
    tensor_list = [
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 403, in <listcomp>
    numpy2lodtensor(b)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 398, in numpy2lodtensor
    lodtensor.set(arr, core.CPUPlace())
ValueError: (InvalidArgument) Input object type error or incompatible array data type. tensor.set() supports array with bool, float16, float32, float64, int8, int16, int32, int64, uint8 or uint16, please check your input or input array data type. (at ../paddle/fluid/pybind/tensor_py.h:565)

Process Process-11:
Traceback (most recent call last):
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 401, in _worker_loop
    tensor_list = [
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 403, in <listcomp>
    numpy2lodtensor(b)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 398, in numpy2lodtensor
    lodtensor.set(arr, core.CPUPlace())
ValueError: (InvalidArgument) Input object type error or incompatible array data type. tensor.set() supports array with bool, float16, float32, float64, int8, int16, int32, int64, uint8 or uint16, please check your input or input array data type. (at ../paddle/fluid/pybind/tensor_py.h:565)

Process Process-4:
Traceback (most recent call last):
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 401, in _worker_loop
    tensor_list = [
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 403, in <listcomp>
    numpy2lodtensor(b)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 398, in numpy2lodtensor
    lodtensor.set(arr, core.CPUPlace())
ValueError: (InvalidArgument) Input object type error or incompatible array data type. tensor.set() supports array with bool, float16, float32, float64, int8, int16, int32, int64, uint8 or uint16, please check your input or input array data type. (at ../paddle/fluid/pybind/tensor_py.h:565)

Process Process-10:
Traceback (most recent call last):
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 401, in _worker_loop
    tensor_list = [
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 403, in <listcomp>
    numpy2lodtensor(b)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 398, in numpy2lodtensor
    lodtensor.set(arr, core.CPUPlace())
ValueError: (InvalidArgument) Input object type error or incompatible array data type. tensor.set() supports array with bool, float16, float32, float64, int8, int16, int32, int64, uint8 or uint16, please check your input or input array data type. (at ../paddle/fluid/pybind/tensor_py.h:565)

Process Process-9:
Traceback (most recent call last):
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 401, in _worker_loop
    tensor_list = [
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 403, in <listcomp>
    numpy2lodtensor(b)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 398, in numpy2lodtensor
    lodtensor.set(arr, core.CPUPlace())
ValueError: (InvalidArgument) Input object type error or incompatible array data type. tensor.set() supports array with bool, float16, float32, float64, int8, int16, int32, int64, uint8 or uint16, please check your input or input array data type. (at ../paddle/fluid/pybind/tensor_py.h:565)

Process Process-3:
Traceback (most recent call last):
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 401, in _worker_loop
    tensor_list = [
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 403, in <listcomp>
    numpy2lodtensor(b)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 398, in numpy2lodtensor
    lodtensor.set(arr, core.CPUPlace())
ValueError: (InvalidArgument) Input object type error or incompatible array data type. tensor.set() supports array with bool, float16, float32, float64, int8, int16, int32, int64, uint8 or uint16, please check your input or input array data type. (at ../paddle/fluid/pybind/tensor_py.h:565)

Process Process-5:
Traceback (most recent call last):
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 401, in _worker_loop
    tensor_list = [
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 403, in <listcomp>
    numpy2lodtensor(b)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 398, in numpy2lodtensor
    lodtensor.set(arr, core.CPUPlace())
ValueError: (InvalidArgument) Input object type error or incompatible array data type. tensor.set() supports array with bool, float16, float32, float64, int8, int16, int32, int64, uint8 or uint16, please check your input or input array data type. (at ../paddle/fluid/pybind/tensor_py.h:565)

Process Process-22:
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 31, in train_epoch
    batch = next(engine.train_dataloader_iter)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/dataloader_iter.py", line 840, in __next__
    self._reader.read_next_list()[0]
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/multiprocess_utils.py", line 133, in __handler__
    core._throw_error_if_process_failed()
SystemError: (Fatal) DataLoader process (pid 277452) exited unexpectedly with code 1. Error detailed are lost due to multiprocessing. Rerunning with:
  1. If run DataLoader by DataLoader.from_generator(...), run with DataLoader.from_generator(..., use_multiprocess=False) may give better error trace.
  2. If run DataLoader by DataLoader(dataset, ...), run with DataLoader(dataset, ..., num_workers=0) may give better error trace (at ../paddle/fluid/imperative/data_loader.cc:150)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 401, in _worker_loop
    tensor_list = [
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 403, in <listcomp>
    numpy2lodtensor(b)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 398, in numpy2lodtensor
    lodtensor.set(arr, core.CPUPlace())
ValueError: (InvalidArgument) Input object type error or incompatible array data type. tensor.set() supports array with bool, float16, float32, float64, int8, int16, int32, int64, uint8 or uint16, please check your input or input array data type. (at ../paddle/fluid/pybind/tensor_py.h:565)

Process Process-21:
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 31, in train_epoch
    batch = next(engine.train_dataloader_iter)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/dataloader_iter.py", line 840, in __next__
    self._reader.read_next_list()[0]
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/multiprocess_utils.py", line 133, in __handler__
    core._throw_error_if_process_failed()
SystemError: (Fatal) DataLoader process (pid 277452) exited unexpectedly with code 1. Error detailed are lost due to multiprocessing. Rerunning with:
  1. If run DataLoader by DataLoader.from_generator(...), run with DataLoader.from_generator(..., use_multiprocess=False) may give better error trace.
  2. If run DataLoader by DataLoader(dataset, ...), run with DataLoader(dataset, ..., num_workers=0) may give better error trace (at ../paddle/fluid/imperative/data_loader.cc:150)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 401, in _worker_loop
    tensor_list = [
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 403, in <listcomp>
    numpy2lodtensor(b)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 398, in numpy2lodtensor
    lodtensor.set(arr, core.CPUPlace())
ValueError: (InvalidArgument) Input object type error or incompatible array data type. tensor.set() supports array with bool, float16, float32, float64, int8, int16, int32, int64, uint8 or uint16, please check your input or input array data type. (at ../paddle/fluid/pybind/tensor_py.h:565)

Process Process-23:
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 31, in train_epoch
    batch = next(engine.train_dataloader_iter)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/dataloader_iter.py", line 840, in __next__
    self._reader.read_next_list()[0]
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/multiprocess_utils.py", line 133, in __handler__
    core._throw_error_if_process_failed()
SystemError: (Fatal) DataLoader process (pid 277452) exited unexpectedly with code 1. Error detailed are lost due to multiprocessing. Rerunning with:
  1. If run DataLoader by DataLoader.from_generator(...), run with DataLoader.from_generator(..., use_multiprocess=False) may give better error trace.
  2. If run DataLoader by DataLoader(dataset, ...), run with DataLoader(dataset, ..., num_workers=0) may give better error trace (at ../paddle/fluid/imperative/data_loader.cc:150)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 401, in _worker_loop
    tensor_list = [
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 403, in <listcomp>
    numpy2lodtensor(b)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 398, in numpy2lodtensor
    lodtensor.set(arr, core.CPUPlace())
ValueError: (InvalidArgument) Input object type error or incompatible array data type. tensor.set() supports array with bool, float16, float32, float64, int8, int16, int32, int64, uint8 or uint16, please check your input or input array data type. (at ../paddle/fluid/pybind/tensor_py.h:565)

Process Process-13:
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 31, in train_epoch
    batch = next(engine.train_dataloader_iter)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/dataloader_iter.py", line 840, in __next__
    self._reader.read_next_list()[0]
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/multiprocess_utils.py", line 133, in __handler__
    core._throw_error_if_process_failed()
SystemError: (Fatal) DataLoader process (pid 277452) exited unexpectedly with code 1. Error detailed are lost due to multiprocessing. Rerunning with:
  1. If run DataLoader by DataLoader.from_generator(...), run with DataLoader.from_generator(..., use_multiprocess=False) may give better error trace.
  2. If run DataLoader by DataLoader(dataset, ...), run with DataLoader(dataset, ..., num_workers=0) may give better error trace (at ../paddle/fluid/imperative/data_loader.cc:150)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 401, in _worker_loop
    tensor_list = [
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 403, in <listcomp>
    numpy2lodtensor(b)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 398, in numpy2lodtensor
    lodtensor.set(arr, core.CPUPlace())
ValueError: (InvalidArgument) Input object type error or incompatible array data type. tensor.set() supports array with bool, float16, float32, float64, int8, int16, int32, int64, uint8 or uint16, please check your input or input array data type. (at ../paddle/fluid/pybind/tensor_py.h:565)

Process Process-14:
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 31, in train_epoch
    batch = next(engine.train_dataloader_iter)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/dataloader_iter.py", line 840, in __next__
    self._reader.read_next_list()[0]
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/multiprocess_utils.py", line 133, in __handler__
    core._throw_error_if_process_failed()
SystemError: (Fatal) DataLoader process (pid 277452) exited unexpectedly with code 1. Error detailed are lost due to multiprocessing. Rerunning with:
  1. If run DataLoader by DataLoader.from_generator(...), run with DataLoader.from_generator(..., use_multiprocess=False) may give better error trace.
  2. If run DataLoader by DataLoader(dataset, ...), run with DataLoader(dataset, ..., num_workers=0) may give better error trace (at ../paddle/fluid/imperative/data_loader.cc:150)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 401, in _worker_loop
    tensor_list = [
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 403, in <listcomp>
    numpy2lodtensor(b)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 398, in numpy2lodtensor
    lodtensor.set(arr, core.CPUPlace())
ValueError: (InvalidArgument) Input object type error or incompatible array data type. tensor.set() supports array with bool, float16, float32, float64, int8, int16, int32, int64, uint8 or uint16, please check your input or input array data type. (at ../paddle/fluid/pybind/tensor_py.h:565)

Process Process-19:
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 31, in train_epoch
    batch = next(engine.train_dataloader_iter)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/dataloader_iter.py", line 840, in __next__
    self._reader.read_next_list()[0]
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/multiprocess_utils.py", line 133, in __handler__
    core._throw_error_if_process_failed()
SystemError: (Fatal) DataLoader process (pid 277452) exited unexpectedly with code 1. Error detailed are lost due to multiprocessing. Rerunning with:
  1. If run DataLoader by DataLoader.from_generator(...), run with DataLoader.from_generator(..., use_multiprocess=False) may give better error trace.
  2. If run DataLoader by DataLoader(dataset, ...), run with DataLoader(dataset, ..., num_workers=0) may give better error trace (at ../paddle/fluid/imperative/data_loader.cc:150)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 401, in _worker_loop
    tensor_list = [
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 403, in <listcomp>
    numpy2lodtensor(b)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 398, in numpy2lodtensor
    lodtensor.set(arr, core.CPUPlace())
ValueError: (InvalidArgument) Input object type error or incompatible array data type. tensor.set() supports array with bool, float16, float32, float64, int8, int16, int32, int64, uint8 or uint16, please check your input or input array data type. (at ../paddle/fluid/pybind/tensor_py.h:565)

Process Process-15:
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 31, in train_epoch
    batch = next(engine.train_dataloader_iter)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/dataloader_iter.py", line 840, in __next__
    self._reader.read_next_list()[0]
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/multiprocess_utils.py", line 133, in __handler__
    core._throw_error_if_process_failed()
SystemError: (Fatal) DataLoader process (pid 277452) exited unexpectedly with code 1. Error detailed are lost due to multiprocessing. Rerunning with:
  1. If run DataLoader by DataLoader.from_generator(...), run with DataLoader.from_generator(..., use_multiprocess=False) may give better error trace.
  2. If run DataLoader by DataLoader(dataset, ...), run with DataLoader(dataset, ..., num_workers=0) may give better error trace (at ../paddle/fluid/imperative/data_loader.cc:150)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 401, in _worker_loop
    tensor_list = [
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 403, in <listcomp>
    numpy2lodtensor(b)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 398, in numpy2lodtensor
    lodtensor.set(arr, core.CPUPlace())
ValueError: (InvalidArgument) Input object type error or incompatible array data type. tensor.set() supports array with bool, float16, float32, float64, int8, int16, int32, int64, uint8 or uint16, please check your input or input array data type. (at ../paddle/fluid/pybind/tensor_py.h:565)

Process Process-18:
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 31, in train_epoch
    batch = next(engine.train_dataloader_iter)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/dataloader_iter.py", line 840, in __next__
    self._reader.read_next_list()[0]
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/multiprocess_utils.py", line 133, in __handler__
    core._throw_error_if_process_failed()
SystemError: (Fatal) DataLoader process (pid 277452) exited unexpectedly with code 1. Error detailed are lost due to multiprocessing. Rerunning with:
  1. If run DataLoader by DataLoader.from_generator(...), run with DataLoader.from_generator(..., use_multiprocess=False) may give better error trace.
  2. If run DataLoader by DataLoader(dataset, ...), run with DataLoader(dataset, ..., num_workers=0) may give better error trace (at ../paddle/fluid/imperative/data_loader.cc:150)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 401, in _worker_loop
    tensor_list = [
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 403, in <listcomp>
    numpy2lodtensor(b)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 398, in numpy2lodtensor
    lodtensor.set(arr, core.CPUPlace())
ValueError: (InvalidArgument) Input object type error or incompatible array data type. tensor.set() supports array with bool, float16, float32, float64, int8, int16, int32, int64, uint8 or uint16, please check your input or input array data type. (at ../paddle/fluid/pybind/tensor_py.h:565)

Process Process-17:
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 31, in train_epoch
    batch = next(engine.train_dataloader_iter)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/dataloader_iter.py", line 840, in __next__
    self._reader.read_next_list()[0]
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/multiprocess_utils.py", line 133, in __handler__
    core._throw_error_if_process_failed()
SystemError: (Fatal) DataLoader process (pid 277452) exited unexpectedly with code 1. Error detailed are lost due to multiprocessing. Rerunning with:
  1. If run DataLoader by DataLoader.from_generator(...), run with DataLoader.from_generator(..., use_multiprocess=False) may give better error trace.
  2. If run DataLoader by DataLoader(dataset, ...), run with DataLoader(dataset, ..., num_workers=0) may give better error trace (at ../paddle/fluid/imperative/data_loader.cc:150)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 401, in _worker_loop
    tensor_list = [
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 403, in <listcomp>
    numpy2lodtensor(b)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 398, in numpy2lodtensor
    lodtensor.set(arr, core.CPUPlace())
ValueError: (InvalidArgument) Input object type error or incompatible array data type. tensor.set() supports array with bool, float16, float32, float64, int8, int16, int32, int64, uint8 or uint16, please check your input or input array data type. (at ../paddle/fluid/pybind/tensor_py.h:565)

Process Process-20:
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 31, in train_epoch
    batch = next(engine.train_dataloader_iter)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/dataloader_iter.py", line 840, in __next__
    self._reader.read_next_list()[0]
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/multiprocess_utils.py", line 133, in __handler__
    core._throw_error_if_process_failed()
SystemError: (Fatal) DataLoader process (pid 277452) exited unexpectedly with code 1. Error detailed are lost due to multiprocessing. Rerunning with:
  1. If run DataLoader by DataLoader.from_generator(...), run with DataLoader.from_generator(..., use_multiprocess=False) may give better error trace.
  2. If run DataLoader by DataLoader(dataset, ...), run with DataLoader(dataset, ..., num_workers=0) may give better error trace (at ../paddle/fluid/imperative/data_loader.cc:150)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 401, in _worker_loop
    tensor_list = [
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 403, in <listcomp>
    numpy2lodtensor(b)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 398, in numpy2lodtensor
    lodtensor.set(arr, core.CPUPlace())
ValueError: (InvalidArgument) Input object type error or incompatible array data type. tensor.set() supports array with bool, float16, float32, float64, int8, int16, int32, int64, uint8 or uint16, please check your input or input array data type. (at ../paddle/fluid/pybind/tensor_py.h:565)

[2025-06-13 19:11:44,879] [ WARNING] dataloader_iter.py:721 - DataLoader 8 workers exit unexpectedly, pids: 277446, 277452, 277460, 277468, 277484, 277492, 277500, 277508
[2025-06-13 19:11:45,996] [ WARNING] dataloader_iter.py:721 - DataLoader 10 workers exit unexpectedly, pids: 277631, 277634, 277636, 277641, 277643, 277645, 277650, 277652, 277654, 277656
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 31, in train_epoch
    batch = next(engine.train_dataloader_iter)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/dataloader_iter.py", line 840, in __next__
    self._reader.read_next_list()[0]
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/multiprocess_utils.py", line 133, in __handler__
    core._throw_error_if_process_failed()
SystemError: (Fatal) DataLoader process (pid 277452) exited unexpectedly with code 1. Error detailed are lost due to multiprocessing. Rerunning with:
  1. If run DataLoader by DataLoader.from_generator(...), run with DataLoader.from_generator(..., use_multiprocess=False) may give better error trace.
  2. If run DataLoader by DataLoader(dataset, ...), run with DataLoader(dataset, ..., num_workers=0) may give better error trace (at ../paddle/fluid/imperative/data_loader.cc:150)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 55, in <module>
    engine.train()
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 344, in train
    self.train_epoch_func(self, epoch_id, print_batch_step)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 37, in train_epoch
    batch = next(engine.train_dataloader_iter)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/dataloader_iter.py", line 840, in __next__
    self._reader.read_next_list()[0]
SystemError: (Fatal) Blocking queue is killed because the data reader raises an exception.
  [Hint: Expected killed_ != true, but received killed_:1 == true:1.] (at ../paddle/phi/core/operators/reader/blocking_queue.h:175)

I0613 19:11:46.334317 277257 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0614 22:30:47.088050 2594440 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0614 22:30:47.088690 2594440 gpu_resources.cc:164] device: 3, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
=======================================================================
I0614 22:30:48.290714 2594440 tcp_utils.cc:111] Retry to connect to 127.0.1.1:56710 while the server is not yet listening.
I0614 22:30:51.290910 2594440 tcp_utils.cc:134] Successfully connected to 127.0.1.1:56710
I0614 22:30:51.345029 2594440 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0614 22:30:51.345054 2594440 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0614 22:30:53.108036 2594440 gpu_resources.cc:306] WARNING: device: 3. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
out1.shape before EMAA: [16, 160, 56, 56]


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Backward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   Conv2dGradNodeFinal::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::conv2d_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&, paddle::Tensor*, paddle::Tensor*)
4   void phi::ConvCudnnGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&, phi::DenseTensor*, phi::DenseTensor*)
5   std::_Function_handler<void (void*), phi::ConvRunner<float, (phi::ConvKind)2>::Apply(phi::GPUContext const&, phi::ConvArgsBase<cudnnContext*, cudnnDataType_t> const&, phi::SearchResult<cudnnConvolutionBwdDataAlgo_t> const&, float const*, float const*, float*, int, int, int, int, unsigned long, phi::DnnWorkspaceHandle*, bool)::{lambda(void*)#1}>::_M_invoke(std::_Any_data const&, void*&&)

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1749911465 (unix time) try "date -d @1749911465" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0x3e80027963d) received by PID 2594440 (TID 0x7d1390554740) from PID 2594365 ***]

/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0615 21:20:12.718011 685854 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0615 21:20:12.718637 685854 gpu_resources.cc:164] device: 3, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
=======================================================================
I0615 21:20:13.927682 685854 tcp_utils.cc:111] Retry to connect to 127.0.1.1:55200 while the server is not yet listening.
I0615 21:20:16.927930 685854 tcp_utils.cc:134] Successfully connected to 127.0.1.1:55200
I0615 21:20:16.960247 685854 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0615 21:20:16.960273 685854 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0615 21:20:18.873471 685854 gpu_resources.cc:306] WARNING: device: 3. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Backward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   Conv2dGradNodeFinal::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::conv2d_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&, paddle::Tensor*, paddle::Tensor*)
4   void phi::ConvCudnnGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&, phi::DenseTensor*, phi::DenseTensor*)
5   std::_Function_handler<void (void*), phi::ConvRunner<float, (phi::ConvKind)2>::Apply(phi::GPUContext const&, phi::ConvArgsBase<cudnnContext*, cudnnDataType_t> const&, phi::SearchResult<cudnnConvolutionBwdDataAlgo_t> const&, float const*, float const*, float*, int, int, int, int, unsigned long, phi::DnnWorkspaceHandle*, bool)::{lambda(void*)#1}>::_M_invoke(std::_Any_data const&, void*&&)

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1749993630 (unix time) try "date -d @1749993630" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0x3e8000a76c6) received by PID 685854 (TID 0x7e191b645740) from PID 685766 ***]

/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0615 21:22:09.148571 689048 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0615 21:22:09.149120 689048 gpu_resources.cc:164] device: 3, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
=======================================================================
I0615 21:22:10.421291 689048 tcp_utils.cc:134] Successfully connected to 127.0.1.1:52364
I0615 21:22:10.464099 689048 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0615 21:22:10.464118 689048 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0615 21:22:12.361905 689048 gpu_resources.cc:306] WARNING: device: 3. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Backward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   Conv2dGradNodeFinal::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::conv2d_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&, paddle::Tensor*, paddle::Tensor*)
4   void phi::ConvCudnnGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&, phi::DenseTensor*, phi::DenseTensor*)
5   std::_Function_handler<void (void*), phi::ConvRunner<float, (phi::ConvKind)2>::Apply(phi::GPUContext const&, phi::ConvArgsBase<cudnnContext*, cudnnDataType_t> const&, phi::SearchResult<cudnnConvolutionBwdDataAlgo_t> const&, float const*, float const*, float*, int, int, int, int, unsigned long, phi::DnnWorkspaceHandle*, bool)::{lambda(void*)#1}>::_M_invoke(std::_Any_data const&, void*&&)

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1749993744 (unix time) try "date -d @1749993744" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0x3e8000a833e) received by PID 689048 (TID 0x7b981657a740) from PID 688958 ***]

/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0615 21:22:35.616822 690060 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0615 21:22:35.617482 690060 gpu_resources.cc:164] device: 3, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
=======================================================================
I0615 21:22:36.891002 690060 tcp_utils.cc:111] Retry to connect to 127.0.1.1:50940 while the server is not yet listening.
I0615 21:22:39.891247 690060 tcp_utils.cc:134] Successfully connected to 127.0.1.1:50940
I0615 21:22:39.926143 690060 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0615 21:22:39.926174 690060 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0615 21:22:41.846585 690060 gpu_resources.cc:306] WARNING: device: 3. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Backward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   Conv2dGradNodeFinal::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::conv2d_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&, paddle::Tensor*, paddle::Tensor*)
4   void phi::ConvCudnnGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&, phi::DenseTensor*, phi::DenseTensor*)
5   phi::SearchResult<cudnnConvolutionBwdDataAlgo_t> phi::SearchAlgorithm<(phi::ConvKind)2>::Find<float>(phi::GPUContext const&, phi::ConvArgsBase<cudnnContext*, cudnnDataType_t> const&, bool, bool, bool)

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1749993773 (unix time) try "date -d @1749993773" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0x3e8000a8733) received by PID 690060 (TID 0x7d503e0ff740) from PID 689971 ***]

