/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0605 20:43:16.142510 3185794 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0605 20:43:16.143159 3185794 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
=======================================================================
I0605 20:43:17.276504 3185794 tcp_utils.cc:111] Retry to connect to 127.0.1.1:46267 while the server is not yet listening.
I0605 20:43:20.276733 3185794 tcp_utils.cc:134] Successfully connected to 127.0.1.1:46267
I0605 20:43:20.320958 3185794 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0605 20:43:20.320986 3185794 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0605 20:43:21.800516 3185794 gpu_resources.cc:306] WARNING: device: 1. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 53, in <module>
    engine.train()
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 360, in train
    acc = self.eval(epoch_id)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/decorator.py", line 232, in fun
    return caller(func, *(extras + args), **kw)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/base/dygraph/base.py", line 400, in _decorate_function
    return func(*args, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 497, in eval
    eval_result = self.eval_func(self, epoch_id)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/evaluation/classification.py", line 62, in classification_eval
    out = engine.model(batch[0], depth=batch[1])
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/distributed/parallel.py", line 561, in forward
    outputs = self._layers(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/model_zoo/repvgg.py", line 346, in forward
    feat1 = self.cross_gsa_modules[1](feat1, rel_pos=geo1)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/base/theseus_layer.py", line 622, in forward
    qk_mat = qk_mat + mask
MemoryError: 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   add_ad_func(paddle::Tensor const&, paddle::Tensor const&)
1   paddle::experimental::add(paddle::Tensor const&, paddle::Tensor const&)
2   void phi::AddRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*)
3   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
4   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
5   paddle::memory::allocation::Allocator::Allocate(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 1. Cannot allocate 4.882812GB memory on GPU 1, 19.911926GB memory has been allocated and available memory is only 3.730957GB.

Please check whether there is any other process using GPU 1.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

I0605 20:44:08.080219 3185794 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0605 20:45:26.047065 3188992 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0605 20:45:26.047688 3188992 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
=======================================================================
I0605 20:45:26.995234 3188992 tcp_utils.cc:134] Successfully connected to 127.0.1.1:48288
I0605 20:45:27.059202 3188992 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0605 20:45:27.059245 3188992 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0605 20:45:28.317525 3188992 gpu_resources.cc:306] WARNING: device: 1. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 53, in <module>
    engine.train()
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 360, in train
    acc = self.eval(epoch_id)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/decorator.py", line 232, in fun
    return caller(func, *(extras + args), **kw)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/base/dygraph/base.py", line 400, in _decorate_function
    return func(*args, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 497, in eval
    eval_result = self.eval_func(self, epoch_id)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/evaluation/classification.py", line 62, in classification_eval
    out = engine.model(batch[0], depth=batch[1])
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/distributed/parallel.py", line 561, in forward
    outputs = self._layers(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/model_zoo/repvgg.py", line 346, in forward
    feat1 = self.cross_gsa_modules[1](feat1, rel_pos=geo1)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/base/theseus_layer.py", line 622, in forward
    qk_mat = qk_mat + mask
MemoryError: 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   add_ad_func(paddle::Tensor const&, paddle::Tensor const&)
1   paddle::experimental::add(paddle::Tensor const&, paddle::Tensor const&)
2   void phi::AddRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*)
3   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
4   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
5   paddle::memory::allocation::Allocator::Allocate(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 1. Cannot allocate 4.882812GB memory on GPU 1, 19.874817GB memory has been allocated and available memory is only 3.768066GB.

Please check whether there is any other process using GPU 1.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

I0605 20:46:22.973196 3188992 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0605 21:37:37.434316 3214890 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0605 21:37:37.434979 3214890 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
=======================================================================
I0605 21:37:38.543394 3214890 tcp_utils.cc:134] Successfully connected to 127.0.1.1:53260
I0605 21:37:38.607165 3214890 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0605 21:37:38.607201 3214890 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0605 21:37:43.020274 3214890 gpu_resources.cc:306] WARNING: device: 1. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 53, in <module>
    engine.train()
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 360, in train
    acc = self.eval(epoch_id)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/decorator.py", line 232, in fun
    return caller(func, *(extras + args), **kw)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/base/dygraph/base.py", line 400, in _decorate_function
    return func(*args, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 497, in eval
    eval_result = self.eval_func(self, epoch_id)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/evaluation/classification.py", line 62, in classification_eval
    out = engine.model(batch[0], depth=batch[1])
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/distributed/parallel.py", line 561, in forward
    outputs = self._layers(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/model_zoo/repvgg.py", line 346, in forward
    feat1 = self.cross_gsa_modules[1](feat1, rel_pos=geo1)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/base/theseus_layer.py", line 622, in forward
    qk_mat = qk_mat + mask
MemoryError: 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   add_ad_func(paddle::Tensor const&, paddle::Tensor const&)
1   paddle::experimental::add(paddle::Tensor const&, paddle::Tensor const&)
2   void phi::AddRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*)
3   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
4   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
5   paddle::memory::allocation::Allocator::Allocate(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 1. Cannot allocate 4.882812GB memory on GPU 1, 19.911926GB memory has been allocated and available memory is only 3.730957GB.

Please check whether there is any other process using GPU 1.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

I0605 21:38:29.189909 3214890 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0605 21:41:31.787025 3218895 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0605 21:41:31.787715 3218895 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
=======================================================================
I0605 21:41:32.901093 3218895 tcp_utils.cc:111] Retry to connect to 127.0.1.1:38907 while the server is not yet listening.
I0605 21:41:35.901327 3218895 tcp_utils.cc:134] Successfully connected to 127.0.1.1:38907
I0605 21:41:35.906203 3218895 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0605 21:41:35.906234 3218895 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0605 21:41:37.428717 3218895 gpu_resources.cc:306] WARNING: device: 1. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 53, in <module>
    engine.train()
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 360, in train
    acc = self.eval(epoch_id)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/decorator.py", line 232, in fun
    return caller(func, *(extras + args), **kw)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/base/dygraph/base.py", line 400, in _decorate_function
    return func(*args, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 497, in eval
    eval_result = self.eval_func(self, epoch_id)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/evaluation/classification.py", line 62, in classification_eval
    out = engine.model(batch[0], depth=batch[1])
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/distributed/parallel.py", line 561, in forward
    outputs = self._layers(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/model_zoo/repvgg.py", line 346, in forward
    feat1 = self.cross_gsa_modules[1](feat1, rel_pos=geo1)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/base/theseus_layer.py", line 622, in forward
    qk_mat = qk_mat + mask
MemoryError: 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   add_ad_func(paddle::Tensor const&, paddle::Tensor const&)
1   paddle::experimental::add(paddle::Tensor const&, paddle::Tensor const&)
2   void phi::AddRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*)
3   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
4   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
5   paddle::memory::allocation::Allocator::Allocate(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 1. Cannot allocate 4.882812GB memory on GPU 1, 19.911926GB memory has been allocated and available memory is only 3.730957GB.

Please check whether there is any other process using GPU 1.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

I0605 21:42:28.582506 3218895 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0605 22:05:45.471670 3244340 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0605 22:05:45.472347 3244340 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
=======================================================================
I0605 22:05:46.647303 3244340 tcp_utils.cc:111] Retry to connect to 127.0.1.1:55308 while the server is not yet listening.
I0605 22:05:49.647538 3244340 tcp_utils.cc:134] Successfully connected to 127.0.1.1:55308
I0605 22:05:49.679273 3244340 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0605 22:05:49.679317 3244340 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0605 22:05:52.745312 3244340 gpu_resources.cc:306] WARNING: device: 1. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 55, in <module>
    engine.train()
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 344, in train
    self.train_epoch_func(self, epoch_id, print_batch_step)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 65, in train_epoch
    out = forward(engine, batch)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 117, in forward
    return engine.model(batch[0],depth=batch[1])
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/distributed/parallel.py", line 561, in forward
    outputs = self._layers(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/model_zoo/repvgg.py", line 346, in forward
    feat1 = self.cross_gsa_modules[1](feat1, rel_pos=geo1)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/base/theseus_layer.py", line 618, in forward
    qk_mat = paddle.matmul(qr, paddle.transpose(kr, perm=[0,1,3,2]))
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/tensor/linalg.py", line 321, in matmul
    return _C_ops.matmul(x, y, transpose_x, transpose_y)
MemoryError: 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_matmul(_object*, _object*, _object*)
1   matmul_ad_func(paddle::Tensor const&, paddle::Tensor const&, bool, bool)
2   paddle::experimental::matmul(paddle::Tensor const&, paddle::Tensor const&, bool, bool)
3   void phi::MatmulKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, bool, bool, phi::DenseTensor*)
4   void phi::MatMulFunctionImplWithBlas<phi::GPUContext, float>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, std::vector<long, std::allocator<long> > const&, std::vector<long, std::allocator<long> > const&, phi::DenseTensor*, bool, bool, bool, phi::funcs::MatmulPlanner*)
5   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
6   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 1. Cannot allocate 9.378906GB memory on GPU 1, 17.357239GB memory has been allocated and available memory is only 6.285645GB.

Please check whether there is any other process using GPU 1.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

I0605 22:06:03.403565 3244340 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0605 22:15:09.220234 3250907 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0605 22:15:09.220878 3250907 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
=======================================================================
I0605 22:15:10.438544 3250907 tcp_utils.cc:134] Successfully connected to 127.0.1.1:48437
I0605 22:15:10.520138 3250907 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0605 22:15:10.520174 3250907 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0605 22:15:14.854387 3250907 gpu_resources.cc:306] WARNING: device: 1. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
I0606 01:00:00.563652 3250907 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0606 09:50:05.982678 4062918 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0606 09:50:05.983284 4062918 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
=======================================================================
I0606 09:50:06.455636 4062918 tcp_utils.cc:134] Successfully connected to 127.0.1.1:35819
I0606 09:50:06.503041 4062918 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0606 09:50:06.503074 4062918 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0606 09:50:08.732975 4062918 gpu_resources.cc:306] WARNING: device: 1. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
I0606 09:52:14.179023 4062918 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0606 09:53:09.815793 4070674 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0606 09:53:09.816345 4070674 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
=======================================================================
I0606 09:53:10.284125 4070674 tcp_utils.cc:111] Retry to connect to 127.0.1.1:57799 while the server is not yet listening.
I0606 09:53:13.284368 4070674 tcp_utils.cc:134] Successfully connected to 127.0.1.1:57799
I0606 09:53:13.333043 4070674 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0606 09:53:13.333072 4070674 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0606 09:53:15.659461 4070674 gpu_resources.cc:306] WARNING: device: 1. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
I0606 12:41:35.535718 4070674 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0606 20:16:15.407837 2970354 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0606 20:16:15.408455 2970354 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
=======================================================================
I0606 20:16:16.467332 2970354 tcp_utils.cc:111] Retry to connect to 127.0.1.1:61030 while the server is not yet listening.
I0606 20:16:19.467514 2970354 tcp_utils.cc:134] Successfully connected to 127.0.1.1:61030
I0606 20:16:19.496094 2970354 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0606 20:16:19.496116 2970354 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0606 20:16:21.789958 2970354 gpu_resources.cc:306] WARNING: device: 1. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Backward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   StackGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::stack_grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, paddle::Tensor const&, int, std::vector<paddle::Tensor*, std::allocator<paddle::Tensor*> >)
4   phi::KernelImpl<void (*)(phi::GPUContext const&, phi::DenseTensor const&, int, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::StackGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, int, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, phi::DenseTensor const&, int, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::funcs::UnStackRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, int, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >*)
6   void phi::funcs::LaunchUnStackKernel<phi::GPUContext, float, int, (phi::funcs::SegmentedArraySize)4>(phi::GPUContext const&, int, int, int, int, phi::DenseTensor const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >*)

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1749212193 (unix time) try "date -d @1749212193" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0x3e8002d510f) received by PID 2970354 (TID 0x7c0150fb5740) from PID 2969871 ***]

/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0606 20:17:28.848647 2988212 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0606 20:17:28.849269 2988212 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
=======================================================================
I0606 20:17:29.967887 2988212 tcp_utils.cc:111] Retry to connect to 127.0.1.1:58310 while the server is not yet listening.
I0606 20:17:32.968101 2988212 tcp_utils.cc:134] Successfully connected to 127.0.1.1:58310
I0606 20:17:33.038059 2988212 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0606 20:17:33.038093 2988212 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0606 20:17:35.205813 2988212 gpu_resources.cc:306] WARNING: device: 1. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
After downsampling, x_reshaped shape: [32, 96, 28, 28], H: 28, W: 28
After downsampling, x_reshaped shape: [32, 96, 28, 28], H: 28, W: 28
After downsampling, x_reshaped shape: [32, 192, 14, 14], H: 14, W: 14
After downsampling, x_reshaped shape: [32, 192, 14, 14], H: 14, W: 14
After downsampling, x_reshaped shape: [32, 192, 14, 14], H: 14, W: 14
After downsampling, x_reshaped shape: [32, 192, 14, 14], H: 14, W: 14
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 768, 3, 3], H: 3, W: 3
After downsampling, x_reshaped shape: [32, 768, 3, 3], H: 3, W: 3


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Backward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   StackGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::stack_grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, paddle::Tensor const&, int, std::vector<paddle::Tensor*, std::allocator<paddle::Tensor*> >)
4   phi::KernelImpl<void (*)(phi::GPUContext const&, phi::DenseTensor const&, int, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::StackGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, int, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, phi::DenseTensor const&, int, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::funcs::UnStackRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, int, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >*)
6   void phi::funcs::LaunchUnStackKernel<phi::GPUContext, float, int, (phi::funcs::SegmentedArraySize)4>(phi::GPUContext const&, int, int, int, int, phi::DenseTensor const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >*)

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1749212267 (unix time) try "date -d @1749212267" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0x3e8002d9709) received by PID 2988212 (TID 0x7fcd7e522740) from PID 2987785 ***]

/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0606 20:35:10.512609 3248825 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0606 20:35:10.513312 3248825 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
=======================================================================
I0606 20:35:10.985713 3248825 tcp_utils.cc:111] Retry to connect to 127.0.1.1:61075 while the server is not yet listening.
I0606 20:35:13.985932 3248825 tcp_utils.cc:134] Successfully connected to 127.0.1.1:61075
I0606 20:35:14.062040 3248825 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0606 20:35:14.062063 3248825 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0606 20:35:16.460433 3248825 gpu_resources.cc:306] WARNING: device: 1. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
feat2.shape after GSA: [16, 56, 56, 160]


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Backward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   Conv2dGradNodeFinal::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::conv2d_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&, paddle::Tensor*, paddle::Tensor*)
4   void phi::ConvCudnnGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&, phi::DenseTensor*, phi::DenseTensor*)
5   std::_Function_handler<void (void*), phi::ConvRunner<float, (phi::ConvKind)2>::Apply(phi::GPUContext const&, phi::ConvArgsBase<cudnnContext*, cudnnDataType_t> const&, phi::SearchResult<cudnnConvolutionBwdDataAlgo_t> const&, float const*, float const*, float*, int, int, int, int, unsigned long, phi::DnnWorkspaceHandle*, bool)::{lambda(void*)#1}>::_M_invoke(std::_Any_data const&, void*&&)

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1749213328 (unix time) try "date -d @1749213328" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0x3e800319136) received by PID 3248825 (TID 0x75b55c771740) from PID 3248438 ***]

/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0606 20:36:20.857314 3257157 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0606 20:36:20.858107 3257157 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
=======================================================================
I0606 20:36:21.362475 3257157 tcp_utils.cc:134] Successfully connected to 127.0.1.1:37936
I0606 20:36:21.431145 3257157 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0606 20:36:21.431174 3257157 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0606 20:36:23.680567 3257157 gpu_resources.cc:306] WARNING: device: 1. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
W0606 20:45:54.176528 3282723 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0606 20:45:54.177134 3282723 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
feat2.shape after GSA: [16, 56, 56, 160]
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
=======================================================================
I0606 20:45:54.706327 3282723 tcp_utils.cc:134] Successfully connected to 127.0.1.1:60980
I0606 20:45:54.783968 3282723 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0606 20:45:54.783986 3282723 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
W0606 20:46:04.456794 3282723 gpu_resources.cc:306] WARNING: device: 1. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_conv2d(_object*, _object*, _object*)
1   conv2d_ad_func(paddle::Tensor const&, paddle::Tensor const&, std::vector<int, std::allocator<int> >, std::vector<int, std::allocator<int> >, std::string, std::vector<int, std::allocator<int> >, int, std::string)
2   paddle::experimental::conv2d(paddle::Tensor const&, paddle::Tensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&)
3   void phi::ConvCudnnKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&, phi::DenseTensor*)
4   void phi::ConvCudnnKernelImplV7<float, phi::GPUContext>(phi::DenseTensor const*, phi::DenseTensor const*, phi::GPUContext const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, phi::backends::gpu::DataLayout, phi::backends::gpu::DataLayout, bool, bool, int, phi::DenseTensor*)
5   phi::DnnWorkspaceHandle::ReallocWorkspace(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1749213971 (unix time) try "date -d @1749213971" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0x3e8003216c8) received by PID 3282723 (TID 0x7117b1aae740) from PID 3282632 ***]

feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
W0606 20:47:25.013118 3287201 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0606 20:47:25.013836 3287201 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
=======================================================================
I0606 20:47:25.552699 3287201 tcp_utils.cc:134] Successfully connected to 127.0.1.1:60650
I0606 20:47:25.630035 3287201 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0606 20:47:25.630060 3287201 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
W0606 20:47:29.566998 3287201 gpu_resources.cc:306] WARNING: device: 1. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 55, in <module>
    engine.train()
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 344, in train
    self.train_epoch_func(self, epoch_id, print_batch_step)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 65, in train_epoch
    out = forward(engine, batch)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 117, in forward
    return engine.model(batch[0],depth=batch[1])
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/distributed/parallel.py", line 561, in forward
    outputs = self._layers(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/model_zoo/repvgg.py", line 329, in forward
    out1 = self.stage1(out)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/container.py", line 754, in forward
    input = layer(input)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/model_zoo/repvgg.py", line 175, in forward
    self.se(self.rbr_dense(inputs) + self.rbr_1x1(inputs) + id_out))
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/model_zoo/repvgg.py", line 81, in forward
    y = self.conv(x)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/conv.py", line 742, in forward
    out = F.conv._conv_nd(
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/functional/conv.py", line 150, in _conv_nd
    pre_bias = _C_ops.conv2d(
MemoryError: 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_conv2d(_object*, _object*, _object*)
1   conv2d_ad_func(paddle::Tensor const&, paddle::Tensor const&, std::vector<int, std::allocator<int> >, std::vector<int, std::allocator<int> >, std::string, std::vector<int, std::allocator<int> >, int, std::string)
2   paddle::experimental::conv2d(paddle::Tensor const&, paddle::Tensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&)
3   void phi::ConvCudnnKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&, phi::DenseTensor*)
4   void phi::ConvCudnnKernelImplV7<float, phi::GPUContext>(phi::DenseTensor const*, phi::DenseTensor const*, phi::GPUContext const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, phi::backends::gpu::DataLayout, phi::backends::gpu::DataLayout, bool, bool, int, phi::DenseTensor*)
5   phi::DnnWorkspaceHandle::ReallocWorkspace(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 1. Cannot allocate 141.328125MB memory on GPU 1, 23.571655GB memory has been allocated and available memory is only 72.937500MB.

Please check whether there is any other process using GPU 1.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

feat2.shape after GSA: [16, 56, 56, 160]
I0606 20:47:40.339169 3287201 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0606 20:48:28.981775 3290687 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0606 20:48:28.982398 3290687 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
=======================================================================
I0606 20:48:29.411042 3290687 tcp_utils.cc:134] Successfully connected to 127.0.1.1:57553
I0606 20:48:29.454047 3290687 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0606 20:48:29.454082 3290687 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
W0606 20:48:36.356731 3290687 gpu_resources.cc:306] WARNING: device: 1. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_conv2d(_object*, _object*, _object*)
1   conv2d_ad_func(paddle::Tensor const&, paddle::Tensor const&, std::vector<int, std::allocator<int> >, std::vector<int, std::allocator<int> >, std::string, std::vector<int, std::allocator<int> >, int, std::string)
2   paddle::experimental::conv2d(paddle::Tensor const&, paddle::Tensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&)
3   void phi::ConvCudnnKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&, phi::DenseTensor*)
4   void phi::ConvCudnnKernelImplV7<float, phi::GPUContext>(phi::DenseTensor const*, phi::DenseTensor const*, phi::GPUContext const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, phi::backends::gpu::DataLayout, phi::backends::gpu::DataLayout, bool, bool, int, phi::DenseTensor*)
5   phi::DnnWorkspaceHandle::ReallocWorkspace(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1749214123 (unix time) try "date -d @1749214123" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0x3e8003235d7) received by PID 3290687 (TID 0x769dbfdfa740) from PID 3290583 ***]

feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
W0606 20:49:55.355449 3294632 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0606 20:49:55.356148 3294632 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
feat2.shape after GSA: [16, 56, 56, 160]
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
=======================================================================
I0606 20:49:55.920857 3294632 tcp_utils.cc:111] Retry to connect to 127.0.1.1:53732 while the server is not yet listening.
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
I0606 20:49:58.921089 3294632 tcp_utils.cc:134] Successfully connected to 127.0.1.1:53732
I0606 20:49:58.944136 3294632 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0606 20:49:58.944180 3294632 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 55, in <module>
    engine.train()
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 331, in train
    metric_info = init_model(self.config.Global, self.model,
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/utils/save_load.py", line 130, in init_model
    assert os.path.exists(checkpoints + ".pdparams"), \
AssertionError: Given dir https://paddle-imagenet-models-name.bj.bcebos.com/dygraph/RepVGG_D2se_pretrained.pdparams.pdparams not exist.
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
I0606 20:49:59.982511 3294632 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
W0606 20:50:33.800736 3296511 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0606 20:50:33.801424 3296511 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
feat2.shape after GSA: [16, 56, 56, 160]
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
=======================================================================
I0606 20:50:34.316283 3296511 tcp_utils.cc:134] Successfully connected to 127.0.1.1:35733
I0606 20:50:34.387063 3296511 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0606 20:50:34.387084 3296511 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 55, in <module>
    engine.train()
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 331, in train
    metric_info = init_model(self.config.Global, self.model,
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/utils/save_load.py", line 130, in init_model
    assert os.path.exists(checkpoints + ".pdparams"), \
AssertionError: Given dir https://paddle-imagenet-models-name.bj.bcebos.com/dygraph/RepVGG_D2se_pretrained.pdparams.pdparams not exist.
I0606 20:50:38.468987 3296511 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
W0606 20:51:56.668202 3300081 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0606 20:51:56.668919 3300081 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
=======================================================================
I0606 20:52:00.645004 3300081 tcp_utils.cc:111] Retry to connect to 127.0.1.1:54280 while the server is not yet listening.
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
I0606 20:52:03.645183 3300081 tcp_utils.cc:134] Successfully connected to 127.0.1.1:54280
I0606 20:52:03.645377 3300081 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0606 20:52:03.645387 3300081 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
feat2.shape after GSA: [16, 56, 56, 160]
W0606 20:52:05.465658 3300081 gpu_resources.cc:306] WARNING: device: 1. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 55, in <module>
    engine.train()
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 344, in train
    self.train_epoch_func(self, epoch_id, print_batch_step)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 65, in train_epoch
    out = forward(engine, batch)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 117, in forward
    return engine.model(batch[0],depth=batch[1])
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/distributed/parallel.py", line 561, in forward
    outputs = self._layers(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/model_zoo/repvgg.py", line 339, in forward
    geo1 = self.geo_prior_gen((H1, W1), depth)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/base/theseus_layer.py", line 502, in forward
    mask = self.generate_pos_decay(H, W)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/base/theseus_layer.py", line 448, in generate_pos_decay
    mask = grid.unsqueeze(1) - grid.unsqueeze(0)
MemoryError: 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   subtract_ad_func(paddle::Tensor const&, paddle::Tensor const&)
1   paddle::experimental::subtract(paddle::Tensor const&, paddle::Tensor const&)
2   void phi::SubtractRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*)
3   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
4   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
5   paddle::memory::allocation::Allocator::Allocate(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 1. Cannot allocate 75.031250MB memory on GPU 1, 23.575562GB memory has been allocated and available memory is only 68.937500MB.

Please check whether there is any other process using GPU 1.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

feat2.shape after GSA: [16, 56, 56, 160]
I0606 20:52:16.238849 3300081 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0606 20:54:22.704269 3307132 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0606 20:54:22.704869 3307132 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')feat2.shape after GSA: [16, 56, 56, 160]

FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
=======================================================================
I0606 20:54:26.738197 3307132 tcp_utils.cc:134] Successfully connected to 127.0.1.1:47225
I0606 20:54:26.820143 3307132 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0606 20:54:26.820190 3307132 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
W0606 20:54:29.126757 3307132 gpu_resources.cc:306] WARNING: device: 1. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 55, in <module>
    engine.train()
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 344, in train
    self.train_epoch_func(self, epoch_id, print_batch_step)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 65, in train_epoch
    out = forward(engine, batch)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 117, in forward
    return engine.model(batch[0],depth=batch[1])
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/distributed/parallel.py", line 561, in forward
    outputs = self._layers(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/model_zoo/repvgg.py", line 339, in forward
    geo1 = self.geo_prior_gen((H1, W1), depth)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/base/theseus_layer.py", line 502, in forward
    mask = self.generate_pos_decay(H, W)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/base/theseus_layer.py", line 448, in generate_pos_decay
    mask = grid.unsqueeze(1) - grid.unsqueeze(0)
MemoryError: 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   subtract_ad_func(paddle::Tensor const&, paddle::Tensor const&)
1   paddle::experimental::subtract(paddle::Tensor const&, paddle::Tensor const&)
2   void phi::SubtractRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*)
3   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
4   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
5   paddle::memory::allocation::Allocator::Allocate(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 1. Cannot allocate 75.031250MB memory on GPU 1, 23.575562GB memory has been allocated and available memory is only 68.937500MB.

Please check whether there is any other process using GPU 1.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

I0606 20:54:39.838794 3307132 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0606 20:59:23.733561 3322145 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0606 20:59:23.734166 3322145 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
=======================================================================
I0606 20:59:27.763432 3322145 tcp_utils.cc:134] Successfully connected to 127.0.1.1:60730
I0606 20:59:27.798975 3322145 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0606 20:59:27.798987 3322145 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
feat2.shape after GSA: [16, 56, 56, 160]
W0606 20:59:29.678714 3322145 gpu_resources.cc:306] WARNING: device: 1. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 55, in <module>
    engine.train()
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 344, in train
    self.train_epoch_func(self, epoch_id, print_batch_step)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 65, in train_epoch
    out = forward(engine, batch)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 117, in forward
    return engine.model(batch[0],depth=batch[1])
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/distributed/parallel.py", line 561, in forward
    outputs = self._layers(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/model_zoo/repvgg.py", line 339, in forward
    geo1 = self.geo_prior_gen((H1, W1), depth)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/base/theseus_layer.py", line 504, in forward
    mask = self.weight[0] * mask + self.weight[1] * mask_d
MemoryError: 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   add_ad_func(paddle::Tensor const&, paddle::Tensor const&)
1   paddle::experimental::add(paddle::Tensor const&, paddle::Tensor const&)
2   void phi::AddRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*)
3   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
4   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
5   paddle::memory::allocation::Allocator::Allocate(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 1. Cannot allocate 1.172363GB memory on GPU 1, 23.276733GB memory has been allocated and available memory is only 374.937500MB.

Please check whether there is any other process using GPU 1.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

feat2.shape after GSA: [16, 56, 56, 160]
I0606 20:59:40.545795 3322145 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0606 21:00:05.135712 3324305 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0606 21:00:05.136344 3324305 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
=======================================================================
I0606 21:00:09.185142 3324305 tcp_utils.cc:134] Successfully connected to 127.0.1.1:61053
I0606 21:00:09.185289 3324305 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0606 21:00:09.185297 3324305 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
W0606 21:00:11.039678 3324305 gpu_resources.cc:306] WARNING: device: 1. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
W0606 21:00:15.385694 3324992 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0606 21:00:15.386286 3324992 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 32, in <module>
    paddle.device.cuda.empty_cache()
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/device/cuda/__init__.py", line 189, in empty_cache
    core.cuda_empty_cache()
OSError: (External) CUDA error(2), out of memory. 
  [Hint: 'cudaErrorMemoryAllocation'. The API call failed because it was unable to allocate enough memory to perform the requested operation. ] (at ../paddle/phi/backends/gpu/cuda/cuda_info.cc:206)

feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 55, in <module>
    engine.train()
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 344, in train
    self.train_epoch_func(self, epoch_id, print_batch_step)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 65, in train_epoch
    out = forward(engine, batch)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 117, in forward
    return engine.model(batch[0],depth=batch[1])
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/distributed/parallel.py", line 561, in forward
    outputs = self._layers(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/model_zoo/repvgg.py", line 339, in forward
    geo1 = self.geo_prior_gen((H1, W1), depth)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/base/theseus_layer.py", line 504, in forward
    mask = self.weight[0] * mask + self.weight[1] * mask_d
MemoryError: 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   add_ad_func(paddle::Tensor const&, paddle::Tensor const&)
1   paddle::experimental::add(paddle::Tensor const&, paddle::Tensor const&)
2   void phi::AddRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*)
3   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
4   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
5   paddle::memory::allocation::Allocator::Allocate(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 1. Cannot allocate 1.172363GB memory on GPU 1, 23.276733GB memory has been allocated and available memory is only 374.937500MB.

Please check whether there is any other process using GPU 1.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

I0606 21:00:21.765309 3324305 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
I0606 21:30:24.931521 3257157 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0607 11:01:32.905318 3800422 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0607 11:01:32.906101 3800422 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
=======================================================================
I0607 11:01:33.363556 3800422 tcp_utils.cc:134] Successfully connected to 127.0.1.1:57706
I0607 11:01:33.363687 3800422 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0607 11:01:33.363695 3800422 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0607 11:01:35.790203 3800422 gpu_resources.cc:306] WARNING: device: 1. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0607 11:18:11.263201 3837229 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0607 11:18:11.263857 3837229 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
=======================================================================
I0607 11:18:11.772697 3837229 tcp_utils.cc:111] Retry to connect to 127.0.1.1:47528 while the server is not yet listening.
I0607 11:18:14.772948 3837229 tcp_utils.cc:134] Successfully connected to 127.0.1.1:47528
I0607 11:18:14.853052 3837229 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0607 11:18:14.853091 3837229 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0607 11:21:53.577736 3842500 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0607 11:21:53.578398 3842500 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
=======================================================================
I0607 11:21:54.063616 3842500 tcp_utils.cc:134] Successfully connected to 127.0.1.1:35213
I0607 11:21:54.092108 3842500 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0607 11:21:54.092149 3842500 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0607 11:21:59.983671 3842500 gpu_resources.cc:306] WARNING: device: 1. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
out1.shape before EMAA: [16, 160, 56, 56]
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 55, in <module>
    engine.train()
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 344, in train
    self.train_epoch_func(self, epoch_id, print_batch_step)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 65, in train_epoch
    out = forward(engine, batch)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 117, in forward
    return engine.model(batch[0],depth=batch[1])
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/distributed/parallel.py", line 561, in forward
    outputs = self._layers(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/model_zoo/repvgg.py", line 354, in forward
    out1 = self.EMAA(out1)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/base/theseus_layer.py", line 684, in forward
    hw = self.conv1x1(paddle.concat([x_h, x_w], axis=2))  # [b*g, c//g, h+w, 1]
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/conv.py", line 742, in forward
    out = F.conv._conv_nd(
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/functional/conv.py", line 150, in _conv_nd
    pre_bias = _C_ops.conv2d(
ValueError: (InvalidArgument) The number of input's channels should be equal to filter's channels * groups for Op(Conv). But received: the input's channels is 160, the input's shape is [16, 160, 112, 1]; the filter's channels is 40, the filter's shape is [40, 40, 1, 1]; the groups is 1, the data_format is NCHW. The error may come from wrong data_format setting.
  [Hint: Expected input_channels == filter_channels * groups, but received input_channels:160 != filter_channels * groups:40.] (at ../paddle/phi/infermeta/binary.cc:651)

I0607 11:22:00.711143 3842500 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0607 11:23:22.756978 3844896 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0607 11:23:22.757928 3844896 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
=======================================================================
I0607 11:23:23.220499 3844896 tcp_utils.cc:111] Retry to connect to 127.0.1.1:42966 while the server is not yet listening.
I0607 11:23:26.220662 3844896 tcp_utils.cc:134] Successfully connected to 127.0.1.1:42966
I0607 11:23:26.222072 3844896 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0607 11:23:26.222101 3844896 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0607 11:23:32.657248 3844896 gpu_resources.cc:306] WARNING: device: 1. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   add_ad_func(paddle::Tensor const&, paddle::Tensor const&)
1   paddle::experimental::add(paddle::Tensor const&, paddle::Tensor const&)
2   void phi::AddRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*)
3   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
4   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
5   paddle::memory::allocation::Allocator::Allocate(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
7   paddle::memory::allocation::RetryAllocator::AllocateImpl(unsigned long)
8   paddle::memory::allocation::StreamSafeCUDAAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
10  paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)
11  paddle::platform::RecordedGpuFree(void*, unsigned long, int)

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1749266680 (unix time) try "date -d @1749266680" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0x3e8003aaa67) received by PID 3844896 (TID 0x79224ddc9740) from PID 3844711 ***]

I0607 11:36:46.656431 3800422 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0607 16:32:59.578603 4002657 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0607 16:32:59.579222 4002657 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
=======================================================================
I0607 16:33:00.056478 4002657 tcp_utils.cc:111] Retry to connect to 127.0.1.1:63984 while the server is not yet listening.
I0607 16:33:03.056715 4002657 tcp_utils.cc:134] Successfully connected to 127.0.1.1:63984
I0607 16:33:03.095229 4002657 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0607 16:33:03.095281 4002657 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0607 16:33:05.487870 4002657 gpu_resources.cc:306] WARNING: device: 1. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0607 16:45:57.711148 4047387 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0607 16:45:57.712077 4047387 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
=======================================================================
I0607 16:45:59.001235 4047387 tcp_utils.cc:134] Successfully connected to 127.0.1.1:53137
I0607 16:45:59.043114 4047387 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0607 16:45:59.043157 4047387 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0607 16:46:01.009835 4047387 gpu_resources.cc:306] WARNING: device: 1. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 55, in <module>
    engine.train()
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 344, in train
    self.train_epoch_func(self, epoch_id, print_batch_step)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 65, in train_epoch
    out = forward(engine, batch)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 117, in forward
    return engine.model(batch[0],depth=batch[1])
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/distributed/parallel.py", line 561, in forward
    outputs = self._layers(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/model_zoo/repvgg.py", line 343, in forward
    geo1 = self.geo_prior_gen((H1, W1), depth)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/base/theseus_layer.py", line 502, in forward
    mask = self.generate_pos_decay(H, W)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/base/theseus_layer.py", line 448, in generate_pos_decay
    mask = grid.unsqueeze(1) - grid.unsqueeze(0)
MemoryError: 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   subtract_ad_func(paddle::Tensor const&, paddle::Tensor const&)
1   paddle::experimental::subtract(paddle::Tensor const&, paddle::Tensor const&)
2   void phi::SubtractRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*)
3   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
4   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
5   paddle::memory::allocation::Allocator::Allocate(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 1. Cannot allocate 75.031250MB memory on GPU 1, 23.593140GB memory has been allocated and available memory is only 50.937500MB.

Please check whether there is any other process using GPU 1.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

I0607 16:46:11.740357 4047387 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0607 16:53:57.679958 4057561 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0607 16:53:57.680590 4057561 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1749286439 (unix time) try "date -d @1749286439" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0x3e8003de98b) received by PID 4057561 (TID 0x7c5bf8b84740) from PID 4057483 ***]

/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0607 16:55:19.921566 4058404 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0607 16:55:19.922224 4058404 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
=======================================================================
I0607 16:55:21.081132 4058404 tcp_utils.cc:134] Successfully connected to 127.0.1.1:43304
I0607 16:55:21.136008 4058404 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0607 16:55:21.136029 4058404 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0607 16:55:22.606352 4058404 gpu_resources.cc:306] WARNING: device: 1. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   void phi::funcs::set_constant_with_place<phi::GPUPlace>(phi::DeviceContext const&, phi::DenseTensor*, float)
1   phi::funcs::SetConstant<phi::GPUContext, float>::operator()(phi::GPUContext const&, phi::DenseTensor*, float)

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1749290350 (unix time) try "date -d @1749290350" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0x3e800011ebc) received by PID 4058404 (TID 0x7eaafca9e740) from PID 73404 ***]

/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0607 18:00:21.725821 74143 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0607 18:00:21.726436 74143 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
=======================================================================
I0607 18:00:22.937130 74143 tcp_utils.cc:111] Retry to connect to 127.0.1.1:44953 while the server is not yet listening.
I0607 18:00:25.937359 74143 tcp_utils.cc:134] Successfully connected to 127.0.1.1:44953
I0607 18:00:25.982213 74143 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0607 18:00:25.982241 74143 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0607 18:00:27.492444 74143 gpu_resources.cc:306] WARNING: device: 1. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   void phi::funcs::set_constant_with_place<phi::GPUPlace>(phi::DeviceContext const&, phi::DenseTensor*, float)
1   phi::funcs::SetConstant<phi::GPUContext, float>::operator()(phi::GPUContext const&, phi::DenseTensor*, float)

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1749293469 (unix time) try "date -d @1749293469" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0x3e80003dfba) received by PID 74143 (TID 0x75cc9cfbe740) from PID 253882 ***]

/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0607 18:56:40.709471 256559 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0607 18:56:40.710052 256559 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
=======================================================================
I0607 18:56:41.913357 256559 tcp_utils.cc:134] Successfully connected to 127.0.1.1:57399
I0607 18:56:41.937024 256559 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0607 18:56:41.937042 256559 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0607 18:56:43.467664 256559 gpu_resources.cc:306] WARNING: device: 1. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
I0607 23:59:32.568549 256559 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0613 18:33:34.811419 248550 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0613 18:33:34.813858 248550 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 54, in <module>
    engine = Engine(config, mode="train")
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 139, in __init__
    self.train_dataloader = build_dataloader(
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/data/__init__.py", line 116, in build_dataloader
    dataset = eval(dataset_name)(**config_dataset)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/data/dataloader/imagenet_dataset.py", line 45, in __init__
    super(ImageNetDataset, self).__init__(image_root, cls_label_path,
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/data/dataloader/common_dataset.py", line 63, in __init__
    self._load_anno()
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/data/dataloader/imagenet_dataset.py", line 85, in _load_anno
    assert os.path.exists(depth_path), f"depth path {depth_path} does not exist."
AssertionError: depth path /mnt/data1_hdd/wgk/PaddleClas/tr/datasetsmuti/train/4/(8)_disp_disp.png does not exist.
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0613 18:41:26.834456 253901 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0613 18:41:26.835079 253901 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 54, in <module>
    engine = Engine(config, mode="train")
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 139, in __init__
    self.train_dataloader = build_dataloader(
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/data/__init__.py", line 116, in build_dataloader
    dataset = eval(dataset_name)(**config_dataset)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/data/dataloader/imagenet_dataset.py", line 45, in __init__
    super(ImageNetDataset, self).__init__(image_root, cls_label_path,
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/data/dataloader/common_dataset.py", line 63, in __init__
    self._load_anno()
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/data/dataloader/imagenet_dataset.py", line 79, in _load_anno
    assert os.path.exists(self.images[
AssertionError: path /mnt/data1_hdd/wgk/PaddleClas/tr/datasetsmuti/train/4/(7).jpg does not exist.
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0613 18:50:24.968458 261829 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0613 18:50:24.969245 261829 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 54, in <module>
    engine = Engine(config, mode="train")
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 213, in __init__
    self.train_metric_func = build_metrics(metric_config)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/metric/__init__.py", line 75, in build_metrics
    metrics_list = CombinedMetrics(copy.deepcopy(config))
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/metric/__init__.py", line 46, in __init__
    eval(metric_name)(**metric_params))
  File "<string>", line 1, in <module>
NameError: name 'Auc' is not defined
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0613 18:53:03.128531 263683 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0613 18:53:03.129238 263683 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 54, in <module>
    engine = Engine(config, mode="train")
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 213, in __init__
    self.train_metric_func = build_metrics(metric_config)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/metric/__init__.py", line 75, in build_metrics
    metrics_list = CombinedMetrics(copy.deepcopy(config))
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/metric/__init__.py", line 46, in __init__
    eval(metric_name)(**metric_params))
TypeError: __init__() got an unexpected keyword argument 'num_classes'
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0613 18:58:14.221460 267205 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0613 18:58:14.222110 267205 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
=======================================================================
I0613 18:58:17.938994 267205 tcp_utils.cc:134] Successfully connected to 127.0.1.1:45901
I0613 18:58:17.968698 267205 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0613 18:58:17.968730 267205 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0613 18:58:20.430318 267205 gpu_resources.cc:306] WARNING: device: 1. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 55, in <module>
    engine.train()
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 344, in train
    self.train_epoch_func(self, epoch_id, print_batch_step)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 65, in train_epoch
    out = forward(engine, batch)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 117, in forward
    return engine.model(batch[0],depth=batch[1])
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/distributed/parallel.py", line 561, in forward
    outputs = self._layers(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/model_zoo/repvgg.py", line 360, in forward
    out3 = self.stage3(out2)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/container.py", line 754, in forward
    input = layer(input)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/model_zoo/repvgg.py", line 176, in forward
    self.se(self.rbr_dense(inputs) + self.rbr_1x1(inputs) + id_out))
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/model_zoo/repvgg.py", line 82, in forward
    y = self.conv(x)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/conv.py", line 742, in forward
    out = F.conv._conv_nd(
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/functional/conv.py", line 150, in _conv_nd
    pre_bias = _C_ops.conv2d(
MemoryError: 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_conv2d(_object*, _object*, _object*)
1   conv2d_ad_func(paddle::Tensor const&, paddle::Tensor const&, std::vector<int, std::allocator<int> >, std::vector<int, std::allocator<int> >, std::string, std::vector<int, std::allocator<int> >, int, std::string)
2   paddle::experimental::conv2d(paddle::Tensor const&, paddle::Tensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&)
3   void phi::ConvCudnnKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&, phi::DenseTensor*)
4   void phi::ConvCudnnKernelImplV7<float, phi::GPUContext>(phi::DenseTensor const*, phi::DenseTensor const*, phi::GPUContext const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, phi::backends::gpu::DataLayout, phi::backends::gpu::DataLayout, bool, bool, int, phi::DenseTensor*)
5   phi::DnnWorkspaceHandle::ReallocWorkspace(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 1. Cannot allocate 29.375244MB memory on GPU 1, 23.628296GB memory has been allocated and available memory is only 14.937500MB.

Please check whether there is any other process using GPU 1.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

I0613 18:58:31.127817 267205 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0613 19:11:33.108395 277253 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0613 19:11:33.109153 277253 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cudnn_batchnorm_spatial_persistent', current_value=True, default_value=False)
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_max_inplace_grad_add', current_value=8, default_value=0)
=======================================================================
I0613 19:11:34.017380 277253 tcp_utils.cc:134] Successfully connected to 127.0.1.1:55550
I0613 19:11:34.062022 277253 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0613 19:11:34.062052 277253 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
Process Process-10:
Traceback (most recent call last):
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 401, in _worker_loop
    tensor_list = [
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 403, in <listcomp>
    numpy2lodtensor(b)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 398, in numpy2lodtensor
    lodtensor.set(arr, core.CPUPlace())
ValueError: (InvalidArgument) Input object type error or incompatible array data type. tensor.set() supports array with bool, float16, float32, float64, int8, int16, int32, int64, uint8 or uint16, please check your input or input array data type. (at ../paddle/fluid/pybind/tensor_py.h:565)

Process Process-3:
Traceback (most recent call last):
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 401, in _worker_loop
    tensor_list = [
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 403, in <listcomp>
    numpy2lodtensor(b)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 398, in numpy2lodtensor
    lodtensor.set(arr, core.CPUPlace())
ValueError: (InvalidArgument) Input object type error or incompatible array data type. tensor.set() supports array with bool, float16, float32, float64, int8, int16, int32, int64, uint8 or uint16, please check your input or input array data type. (at ../paddle/fluid/pybind/tensor_py.h:565)

Process Process-5:
Traceback (most recent call last):
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 401, in _worker_loop
    tensor_list = [
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 403, in <listcomp>
    numpy2lodtensor(b)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 398, in numpy2lodtensor
    lodtensor.set(arr, core.CPUPlace())
ValueError: (InvalidArgument) Input object type error or incompatible array data type. tensor.set() supports array with bool, float16, float32, float64, int8, int16, int32, int64, uint8 or uint16, please check your input or input array data type. (at ../paddle/fluid/pybind/tensor_py.h:565)

Process Process-1:
Traceback (most recent call last):
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 401, in _worker_loop
    tensor_list = [
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 403, in <listcomp>
    numpy2lodtensor(b)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 398, in numpy2lodtensor
    lodtensor.set(arr, core.CPUPlace())
ValueError: (InvalidArgument) Input object type error or incompatible array data type. tensor.set() supports array with bool, float16, float32, float64, int8, int16, int32, int64, uint8 or uint16, please check your input or input array data type. (at ../paddle/fluid/pybind/tensor_py.h:565)

Process Process-2:
Traceback (most recent call last):
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 401, in _worker_loop
    tensor_list = [
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 403, in <listcomp>
    numpy2lodtensor(b)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 398, in numpy2lodtensor
    lodtensor.set(arr, core.CPUPlace())
ValueError: (InvalidArgument) Input object type error or incompatible array data type. tensor.set() supports array with bool, float16, float32, float64, int8, int16, int32, int64, uint8 or uint16, please check your input or input array data type. (at ../paddle/fluid/pybind/tensor_py.h:565)

Process Process-4:
Traceback (most recent call last):
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 401, in _worker_loop
    tensor_list = [
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 403, in <listcomp>
    numpy2lodtensor(b)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 398, in numpy2lodtensor
    lodtensor.set(arr, core.CPUPlace())
ValueError: (InvalidArgument) Input object type error or incompatible array data type. tensor.set() supports array with bool, float16, float32, float64, int8, int16, int32, int64, uint8 or uint16, please check your input or input array data type. (at ../paddle/fluid/pybind/tensor_py.h:565)

Process Process-11:
Traceback (most recent call last):
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 401, in _worker_loop
    tensor_list = [
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 403, in <listcomp>
    numpy2lodtensor(b)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 398, in numpy2lodtensor
    lodtensor.set(arr, core.CPUPlace())
ValueError: (InvalidArgument) Input object type error or incompatible array data type. tensor.set() supports array with bool, float16, float32, float64, int8, int16, int32, int64, uint8 or uint16, please check your input or input array data type. (at ../paddle/fluid/pybind/tensor_py.h:565)

Process Process-8:
Traceback (most recent call last):
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 401, in _worker_loop
    tensor_list = [
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 403, in <listcomp>
    numpy2lodtensor(b)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 398, in numpy2lodtensor
    lodtensor.set(arr, core.CPUPlace())
ValueError: (InvalidArgument) Input object type error or incompatible array data type. tensor.set() supports array with bool, float16, float32, float64, int8, int16, int32, int64, uint8 or uint16, please check your input or input array data type. (at ../paddle/fluid/pybind/tensor_py.h:565)

Process Process-9:
Traceback (most recent call last):
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 401, in _worker_loop
    tensor_list = [
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 403, in <listcomp>
    numpy2lodtensor(b)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 398, in numpy2lodtensor
    lodtensor.set(arr, core.CPUPlace())
ValueError: (InvalidArgument) Input object type error or incompatible array data type. tensor.set() supports array with bool, float16, float32, float64, int8, int16, int32, int64, uint8 or uint16, please check your input or input array data type. (at ../paddle/fluid/pybind/tensor_py.h:565)

Process Process-7:
Traceback (most recent call last):
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 401, in _worker_loop
    tensor_list = [
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 403, in <listcomp>
    numpy2lodtensor(b)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 398, in numpy2lodtensor
    lodtensor.set(arr, core.CPUPlace())
ValueError: (InvalidArgument) Input object type error or incompatible array data type. tensor.set() supports array with bool, float16, float32, float64, int8, int16, int32, int64, uint8 or uint16, please check your input or input array data type. (at ../paddle/fluid/pybind/tensor_py.h:565)

Process Process-6:
Traceback (most recent call last):
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 401, in _worker_loop
    tensor_list = [
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 403, in <listcomp>
    numpy2lodtensor(b)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 398, in numpy2lodtensor
    lodtensor.set(arr, core.CPUPlace())
ValueError: (InvalidArgument) Input object type error or incompatible array data type. tensor.set() supports array with bool, float16, float32, float64, int8, int16, int32, int64, uint8 or uint16, please check your input or input array data type. (at ../paddle/fluid/pybind/tensor_py.h:565)

[2025-06-13 19:11:45,029] [ WARNING] dataloader_iter.py:721 - DataLoader 11 workers exit unexpectedly, pids: 277434, 277442, 277450, 277458, 277466, 277475, 277481, 277488, 277496, 277504, 277512
Process Process-22:
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 31, in train_epoch
    batch = next(engine.train_dataloader_iter)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/dataloader_iter.py", line 840, in __next__
    self._reader.read_next_list()[0]
SystemError: (Fatal) Blocking queue is killed because the data reader raises an exception.
  [Hint: Expected killed_ != true, but received killed_:1 == true:1.] (at ../paddle/phi/core/operators/reader/blocking_queue.h:175)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 401, in _worker_loop
    tensor_list = [
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 403, in <listcomp>
    numpy2lodtensor(b)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 398, in numpy2lodtensor
    lodtensor.set(arr, core.CPUPlace())
ValueError: (InvalidArgument) Input object type error or incompatible array data type. tensor.set() supports array with bool, float16, float32, float64, int8, int16, int32, int64, uint8 or uint16, please check your input or input array data type. (at ../paddle/fluid/pybind/tensor_py.h:565)

Process Process-16:
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 31, in train_epoch
    batch = next(engine.train_dataloader_iter)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/dataloader_iter.py", line 840, in __next__
    self._reader.read_next_list()[0]
SystemError: (Fatal) Blocking queue is killed because the data reader raises an exception.
  [Hint: Expected killed_ != true, but received killed_:1 == true:1.] (at ../paddle/phi/core/operators/reader/blocking_queue.h:175)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 401, in _worker_loop
    tensor_list = [
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 403, in <listcomp>
    numpy2lodtensor(b)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 398, in numpy2lodtensor
    lodtensor.set(arr, core.CPUPlace())
ValueError: (InvalidArgument) Input object type error or incompatible array data type. tensor.set() supports array with bool, float16, float32, float64, int8, int16, int32, int64, uint8 or uint16, please check your input or input array data type. (at ../paddle/fluid/pybind/tensor_py.h:565)

Process Process-15:
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 31, in train_epoch
    batch = next(engine.train_dataloader_iter)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/dataloader_iter.py", line 840, in __next__
    self._reader.read_next_list()[0]
SystemError: (Fatal) Blocking queue is killed because the data reader raises an exception.
  [Hint: Expected killed_ != true, but received killed_:1 == true:1.] (at ../paddle/phi/core/operators/reader/blocking_queue.h:175)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 401, in _worker_loop
    tensor_list = [
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 403, in <listcomp>
    numpy2lodtensor(b)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 398, in numpy2lodtensor
    lodtensor.set(arr, core.CPUPlace())
ValueError: (InvalidArgument) Input object type error or incompatible array data type. tensor.set() supports array with bool, float16, float32, float64, int8, int16, int32, int64, uint8 or uint16, please check your input or input array data type. (at ../paddle/fluid/pybind/tensor_py.h:565)

Process Process-20:
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 31, in train_epoch
    batch = next(engine.train_dataloader_iter)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/dataloader_iter.py", line 840, in __next__
    self._reader.read_next_list()[0]
SystemError: (Fatal) Blocking queue is killed because the data reader raises an exception.
  [Hint: Expected killed_ != true, but received killed_:1 == true:1.] (at ../paddle/phi/core/operators/reader/blocking_queue.h:175)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 401, in _worker_loop
    tensor_list = [
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 403, in <listcomp>
    numpy2lodtensor(b)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 398, in numpy2lodtensor
    lodtensor.set(arr, core.CPUPlace())
ValueError: (InvalidArgument) Input object type error or incompatible array data type. tensor.set() supports array with bool, float16, float32, float64, int8, int16, int32, int64, uint8 or uint16, please check your input or input array data type. (at ../paddle/fluid/pybind/tensor_py.h:565)

Process Process-19:
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 31, in train_epoch
    batch = next(engine.train_dataloader_iter)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/dataloader_iter.py", line 840, in __next__
    self._reader.read_next_list()[0]
SystemError: (Fatal) Blocking queue is killed because the data reader raises an exception.
  [Hint: Expected killed_ != true, but received killed_:1 == true:1.] (at ../paddle/phi/core/operators/reader/blocking_queue.h:175)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 401, in _worker_loop
    tensor_list = [
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 403, in <listcomp>
    numpy2lodtensor(b)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 398, in numpy2lodtensor
    lodtensor.set(arr, core.CPUPlace())
ValueError: (InvalidArgument) Input object type error or incompatible array data type. tensor.set() supports array with bool, float16, float32, float64, int8, int16, int32, int64, uint8 or uint16, please check your input or input array data type. (at ../paddle/fluid/pybind/tensor_py.h:565)

Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 31, in train_epoch
    batch = next(engine.train_dataloader_iter)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/dataloader_iter.py", line 840, in __next__
    self._reader.read_next_list()[0]
SystemError: (Fatal) Blocking queue is killed because the data reader raises an exception.
  [Hint: Expected killed_ != true, but received killed_:1 == true:1.] (at ../paddle/phi/core/operators/reader/blocking_queue.h:175)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 55, in <module>
    engine.train()
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 344, in train
    self.train_epoch_func(self, epoch_id, print_batch_step)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 37, in train_epoch
    batch = next(engine.train_dataloader_iter)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/dataloader_iter.py", line 840, in __next__
    self._reader.read_next_list()[0]
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/multiprocess_utils.py", line 133, in __handler__
    core._throw_error_if_process_failed()
SystemError: (Fatal) DataLoader process (pid 278030) exited unexpectedly with code 1. Error detailed are lost due to multiprocessing. Rerunning with:
  1. If run DataLoader by DataLoader.from_generator(...), run with DataLoader.from_generator(..., use_multiprocess=False) may give better error trace.
  2. If run DataLoader by DataLoader(dataset, ...), run with DataLoader(dataset, ..., num_workers=0) may give better error trace (at ../paddle/fluid/imperative/data_loader.cc:150)

I0613 19:11:46.307741 277253 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0614 22:30:47.105573 2594436 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0614 22:30:47.106235 2594436 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
=======================================================================
I0614 22:30:48.317067 2594436 tcp_utils.cc:111] Retry to connect to 127.0.1.1:56710 while the server is not yet listening.
I0614 22:30:51.317256 2594436 tcp_utils.cc:134] Successfully connected to 127.0.1.1:56710
I0614 22:30:51.345033 2594436 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0614 22:30:51.345050 2594436 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0614 22:30:53.110350 2594436 gpu_resources.cc:306] WARNING: device: 1. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 55, in <module>
    engine.train()
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 344, in train
    self.train_epoch_func(self, epoch_id, print_batch_step)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 65, in train_epoch
    out = forward(engine, batch)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 117, in forward
    return engine.model(batch[0],depth=batch[1])
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/distributed/parallel.py", line 561, in forward
    outputs = self._layers(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/model_zoo/repvgg.py", line 343, in forward
    geo1 = self.geo_prior_gen((H1, W1), depth)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/base/theseus_layer.py", line 504, in forward
    mask = self.weight[0] * mask + self.weight[1] * mask_d
MemoryError: 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   add_ad_func(paddle::Tensor const&, paddle::Tensor const&)
1   paddle::experimental::add(paddle::Tensor const&, paddle::Tensor const&)
2   void phi::AddRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*)
3   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
4   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
5   paddle::memory::allocation::Allocator::Allocate(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 1. Cannot allocate 2.344727GB memory on GPU 1, 22.147827GB memory has been allocated and available memory is only 1.495056GB.

Please check whether there is any other process using GPU 1.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

I0614 22:31:03.716372 2594436 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0615 21:20:12.673384 685850 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0615 21:20:12.673944 685850 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
=======================================================================
I0615 21:20:13.911870 685850 tcp_utils.cc:111] Retry to connect to 127.0.1.1:55200 while the server is not yet listening.
I0615 21:20:16.912101 685850 tcp_utils.cc:134] Successfully connected to 127.0.1.1:55200
I0615 21:20:16.960258 685850 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0615 21:20:16.960296 685850 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0615 21:20:18.878885 685850 gpu_resources.cc:306] WARNING: device: 1. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 55, in <module>
    engine.train()
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 344, in train
    self.train_epoch_func(self, epoch_id, print_batch_step)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 65, in train_epoch
    out = forward(engine, batch)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 117, in forward
    return engine.model(batch[0],depth=batch[1])
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/distributed/parallel.py", line 561, in forward
    outputs = self._layers(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/model_zoo/repvgg.py", line 334, in forward
    out1 = self.stage1(out)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/container.py", line 754, in forward
    input = layer(input)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/model_zoo/repvgg.py", line 176, in forward
    self.se(self.rbr_dense(inputs) + self.rbr_1x1(inputs) + id_out))
MemoryError: 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   add_ad_func(paddle::Tensor const&, paddle::Tensor const&)
1   paddle::experimental::add(paddle::Tensor const&, paddle::Tensor const&)
2   void phi::AddRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*)
3   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
4   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
5   paddle::memory::allocation::Allocator::Allocate(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 1. Cannot allocate 61.250000MB memory on GPU 1, 23.606812GB memory has been allocated and available memory is only 36.937500MB.

Please check whether there is any other process using GPU 1.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

I0615 21:20:29.494817 685850 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0615 21:22:09.147830 689044 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0615 21:22:09.148591 689044 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
=======================================================================
I0615 21:22:10.408391 689044 tcp_utils.cc:134] Successfully connected to 127.0.1.1:52364
I0615 21:22:10.464176 689044 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0615 21:22:10.464210 689044 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0615 21:22:12.360590 689044 gpu_resources.cc:306] WARNING: device: 1. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 55, in <module>
    engine.train()
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 344, in train
    self.train_epoch_func(self, epoch_id, print_batch_step)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 65, in train_epoch
    out = forward(engine, batch)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 117, in forward
    return engine.model(batch[0],depth=batch[1])
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/distributed/parallel.py", line 561, in forward
    outputs = self._layers(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/model_zoo/repvgg.py", line 334, in forward
    out1 = self.stage1(out)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/container.py", line 754, in forward
    input = layer(input)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/model_zoo/repvgg.py", line 176, in forward
    self.se(self.rbr_dense(inputs) + self.rbr_1x1(inputs) + id_out))
MemoryError: 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   add_ad_func(paddle::Tensor const&, paddle::Tensor const&)
1   paddle::experimental::add(paddle::Tensor const&, paddle::Tensor const&)
2   void phi::AddRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*)
3   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
4   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
5   paddle::memory::allocation::Allocator::Allocate(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 1. Cannot allocate 61.250000MB memory on GPU 1, 23.606812GB memory has been allocated and available memory is only 36.937500MB.

Please check whether there is any other process using GPU 1.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

I0615 21:22:22.995244 689044 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0615 21:22:35.612450 690056 gpu_resources.cc:119] Please NOTE: device: 1, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0615 21:22:35.613096 690056 gpu_resources.cc:164] device: 1, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='1', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
=======================================================================
I0615 21:22:36.907451 690056 tcp_utils.cc:134] Successfully connected to 127.0.1.1:50940
I0615 21:22:36.974118 690056 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0615 21:22:36.974157 690056 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0615 21:22:41.853662 690056 gpu_resources.cc:306] WARNING: device: 1. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 55, in <module>
    engine.train()
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 344, in train
    self.train_epoch_func(self, epoch_id, print_batch_step)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 65, in train_epoch
    out = forward(engine, batch)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 117, in forward
    return engine.model(batch[0],depth=batch[1])
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/distributed/parallel.py", line 561, in forward
    outputs = self._layers(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/model_zoo/repvgg.py", line 334, in forward
    out1 = self.stage1(out)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/container.py", line 754, in forward
    input = layer(input)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/model_zoo/repvgg.py", line 176, in forward
    self.se(self.rbr_dense(inputs) + self.rbr_1x1(inputs) + id_out))
MemoryError: 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   add_ad_func(paddle::Tensor const&, paddle::Tensor const&)
1   paddle::experimental::add(paddle::Tensor const&, paddle::Tensor const&)
2   void phi::AddRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*)
3   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
4   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
5   paddle::memory::allocation::Allocator::Allocate(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 1. Cannot allocate 61.250000MB memory on GPU 1, 23.606812GB memory has been allocated and available memory is only 36.937500MB.

Please check whether there is any other process using GPU 1.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

I0615 21:22:52.443239 690056 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0615 21:25:35.766292 694791 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0615 21:25:35.767056 694791 gpu_resources.cc:164] device: 3, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
=======================================================================
I0615 21:25:36.868552 694791 tcp_utils.cc:134] Successfully connected to 127.0.1.1:44852
I0615 21:25:36.933077 694791 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0615 21:25:36.933117 694791 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0615 21:25:38.631127 694791 gpu_resources.cc:306] WARNING: device: 3. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0615 21:26:19.630164 696465 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0615 21:26:19.630864 696465 gpu_resources.cc:164] device: 3, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
=======================================================================
I0615 21:26:23.082820 696465 tcp_utils.cc:134] Successfully connected to 127.0.1.1:39078
I0615 21:26:23.101980 696465 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0615 21:26:23.101998 696465 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0615 21:26:24.991281 696465 gpu_resources.cc:306] WARNING: device: 3. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0615 21:29:10.411862 702430 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0615 21:29:10.412490 702430 gpu_resources.cc:164] device: 3, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
=======================================================================
I0615 21:29:11.549135 702430 tcp_utils.cc:134] Successfully connected to 127.0.1.1:35282
I0615 21:29:11.608016 702430 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0615 21:29:11.608052 702430 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0615 21:29:13.367138 702430 gpu_resources.cc:306] WARNING: device: 3. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 55, in <module>
    engine.train()
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 344, in train
    self.train_epoch_func(self, epoch_id, print_batch_step)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 65, in train_epoch
    out = forward(engine, batch)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 117, in forward
    return engine.model(batch[0],depth=batch[1])
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/distributed/parallel.py", line 561, in forward
    outputs = self._layers(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/model_zoo/repvgg.py", line 358, in forward
    out2 = self.stage2(out1)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/container.py", line 754, in forward
    input = layer(input)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/model_zoo/repvgg.py", line 176, in forward
    self.se(self.rbr_dense(inputs) + self.rbr_1x1(inputs) + id_out))
MemoryError: 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   add_ad_func(paddle::Tensor const&, paddle::Tensor const&)
1   paddle::experimental::add(paddle::Tensor const&, paddle::Tensor const&)
2   void phi::AddRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*)
3   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
4   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
5   paddle::memory::allocation::Allocator::Allocate(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 3. Cannot allocate 30.625000MB memory on GPU 3, 23.630127GB memory has been allocated and available memory is only 10.312500MB.

Please check whether there is any other process using GPU 3.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

I0615 21:29:23.917163 702430 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0615 21:31:32.508750 706452 gpu_resources.cc:119] Please NOTE: device: 3, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0615 21:31:32.509348 706452 gpu_resources.cc:164] device: 3, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='3', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
=======================================================================
I0615 21:31:33.622751 706452 tcp_utils.cc:134] Successfully connected to 127.0.1.1:44474
I0615 21:31:33.642104 706452 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0615 21:31:33.642139 706452 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0615 21:31:35.319168 706452 gpu_resources.cc:306] WARNING: device: 3. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
I0617 04:15:44.113626 706452 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
