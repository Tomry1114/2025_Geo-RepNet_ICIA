/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0605 20:43:16.142452 3185796 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0605 20:43:16.143129 3185796 gpu_resources.cc:164] device: 2, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
=======================================================================
I0605 20:43:17.276099 3185796 tcp_utils.cc:111] Retry to connect to 127.0.1.1:46267 while the server is not yet listening.
I0605 20:43:20.276342 3185796 tcp_utils.cc:134] Successfully connected to 127.0.1.1:46267
I0605 20:43:20.320923 3185796 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0605 20:43:20.320953 3185796 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0605 20:43:21.803290 3185796 gpu_resources.cc:306] WARNING: device: 2. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 53, in <module>
    engine.train()
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 360, in train
    acc = self.eval(epoch_id)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/decorator.py", line 232, in fun
    return caller(func, *(extras + args), **kw)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/base/dygraph/base.py", line 400, in _decorate_function
    return func(*args, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 497, in eval
    eval_result = self.eval_func(self, epoch_id)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/evaluation/classification.py", line 62, in classification_eval
    out = engine.model(batch[0], depth=batch[1])
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/distributed/parallel.py", line 561, in forward
    outputs = self._layers(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/model_zoo/repvgg.py", line 346, in forward
    feat1 = self.cross_gsa_modules[1](feat1, rel_pos=geo1)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/base/theseus_layer.py", line 622, in forward
    qk_mat = qk_mat + mask
MemoryError: 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   add_ad_func(paddle::Tensor const&, paddle::Tensor const&)
1   paddle::experimental::add(paddle::Tensor const&, paddle::Tensor const&)
2   void phi::AddRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*)
3   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
4   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
5   paddle::memory::allocation::Allocator::Allocate(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 2. Cannot allocate 4.882812GB memory on GPU 2, 19.911926GB memory has been allocated and available memory is only 3.730957GB.

Please check whether there is any other process using GPU 2.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

I0605 20:44:08.067276 3185796 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0605 21:37:37.434248 3214892 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0605 21:37:37.434978 3214892 gpu_resources.cc:164] device: 2, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
=======================================================================
I0605 21:37:38.545078 3214892 tcp_utils.cc:134] Successfully connected to 127.0.1.1:53260
I0605 21:37:38.607198 3214892 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0605 21:37:38.607232 3214892 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0605 21:37:43.019179 3214892 gpu_resources.cc:306] WARNING: device: 2. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 53, in <module>
    engine.train()
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 360, in train
    acc = self.eval(epoch_id)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/decorator.py", line 232, in fun
    return caller(func, *(extras + args), **kw)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/base/dygraph/base.py", line 400, in _decorate_function
    return func(*args, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 497, in eval
    eval_result = self.eval_func(self, epoch_id)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/evaluation/classification.py", line 62, in classification_eval
    out = engine.model(batch[0], depth=batch[1])
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/distributed/parallel.py", line 561, in forward
    outputs = self._layers(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/model_zoo/repvgg.py", line 346, in forward
    feat1 = self.cross_gsa_modules[1](feat1, rel_pos=geo1)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/base/theseus_layer.py", line 622, in forward
    qk_mat = qk_mat + mask
MemoryError: 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   add_ad_func(paddle::Tensor const&, paddle::Tensor const&)
1   paddle::experimental::add(paddle::Tensor const&, paddle::Tensor const&)
2   void phi::AddRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*)
3   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
4   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
5   paddle::memory::allocation::Allocator::Allocate(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 2. Cannot allocate 4.882812GB memory on GPU 2, 19.911926GB memory has been allocated and available memory is only 3.730957GB.

Please check whether there is any other process using GPU 2.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

I0605 21:38:29.022287 3214892 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0605 21:41:31.822770 3218897 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0605 21:41:31.823444 3218897 gpu_resources.cc:164] device: 2, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
=======================================================================
I0605 21:41:32.917788 3218897 tcp_utils.cc:111] Retry to connect to 127.0.1.1:38907 while the server is not yet listening.
I0605 21:41:35.918027 3218897 tcp_utils.cc:134] Successfully connected to 127.0.1.1:38907
I0605 21:41:35.998154 3218897 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0605 21:41:35.998193 3218897 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0605 21:41:37.340894 3218897 gpu_resources.cc:306] WARNING: device: 2. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 53, in <module>
    engine.train()
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 360, in train
    acc = self.eval(epoch_id)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/decorator.py", line 232, in fun
    return caller(func, *(extras + args), **kw)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/base/dygraph/base.py", line 400, in _decorate_function
    return func(*args, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 497, in eval
    eval_result = self.eval_func(self, epoch_id)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/evaluation/classification.py", line 62, in classification_eval
    out = engine.model(batch[0], depth=batch[1])
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/distributed/parallel.py", line 561, in forward
    outputs = self._layers(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/model_zoo/repvgg.py", line 346, in forward
    feat1 = self.cross_gsa_modules[1](feat1, rel_pos=geo1)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/base/theseus_layer.py", line 622, in forward
    qk_mat = qk_mat + mask
MemoryError: 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   add_ad_func(paddle::Tensor const&, paddle::Tensor const&)
1   paddle::experimental::add(paddle::Tensor const&, paddle::Tensor const&)
2   void phi::AddRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*)
3   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
4   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
5   paddle::memory::allocation::Allocator::Allocate(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 2. Cannot allocate 4.882812GB memory on GPU 2, 19.911926GB memory has been allocated and available memory is only 3.730957GB.

Please check whether there is any other process using GPU 2.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

I0605 21:42:28.517688 3218897 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0605 22:05:45.452206 3244342 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0605 22:05:45.452817 3244342 gpu_resources.cc:164] device: 2, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
=======================================================================
I0605 22:05:46.649032 3244342 tcp_utils.cc:111] Retry to connect to 127.0.1.1:55308 while the server is not yet listening.
I0605 22:05:49.649180 3244342 tcp_utils.cc:134] Successfully connected to 127.0.1.1:55308
I0605 22:05:49.679322 3244342 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0605 22:05:49.679355 3244342 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0605 22:05:52.746760 3244342 gpu_resources.cc:306] WARNING: device: 2. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 55, in <module>
    engine.train()
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 344, in train
    self.train_epoch_func(self, epoch_id, print_batch_step)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 65, in train_epoch
    out = forward(engine, batch)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 117, in forward
    return engine.model(batch[0],depth=batch[1])
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/distributed/parallel.py", line 561, in forward
    outputs = self._layers(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/model_zoo/repvgg.py", line 346, in forward
    feat1 = self.cross_gsa_modules[1](feat1, rel_pos=geo1)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/base/theseus_layer.py", line 618, in forward
    qk_mat = paddle.matmul(qr, paddle.transpose(kr, perm=[0,1,3,2]))
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/tensor/linalg.py", line 321, in matmul
    return _C_ops.matmul(x, y, transpose_x, transpose_y)
MemoryError: 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_matmul(_object*, _object*, _object*)
1   matmul_ad_func(paddle::Tensor const&, paddle::Tensor const&, bool, bool)
2   paddle::experimental::matmul(paddle::Tensor const&, paddle::Tensor const&, bool, bool)
3   void phi::MatmulKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, bool, bool, phi::DenseTensor*)
4   void phi::MatMulFunctionImplWithBlas<phi::GPUContext, float>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, std::vector<long, std::allocator<long> > const&, std::vector<long, std::allocator<long> > const&, phi::DenseTensor*, bool, bool, bool, phi::funcs::MatmulPlanner*)
5   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
6   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 2. Cannot allocate 9.378906GB memory on GPU 2, 17.357239GB memory has been allocated and available memory is only 6.285645GB.

Please check whether there is any other process using GPU 2.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

I0605 22:06:03.419358 3244342 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0605 22:15:09.218767 3250909 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0605 22:15:09.219417 3250909 gpu_resources.cc:164] device: 2, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
=======================================================================
I0605 22:15:10.392168 3250909 tcp_utils.cc:111] Retry to connect to 127.0.1.1:48437 while the server is not yet listening.
I0605 22:15:13.392414 3250909 tcp_utils.cc:134] Successfully connected to 127.0.1.1:48437
I0605 22:15:13.428211 3250909 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0605 22:15:13.428257 3250909 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0605 22:15:14.751384 3250909 gpu_resources.cc:306] WARNING: device: 2. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
I0606 01:00:00.634327 3250909 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0606 09:50:05.985138 4062923 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0606 09:50:05.985776 4062923 gpu_resources.cc:164] device: 2, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
=======================================================================
I0606 09:50:06.481014 4062923 tcp_utils.cc:134] Successfully connected to 127.0.1.1:35819
I0606 09:50:06.503232 4062923 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0606 09:50:06.503266 4062923 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0606 09:50:08.800251 4062923 gpu_resources.cc:306] WARNING: device: 2. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
I0606 09:52:14.212772 4062923 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0606 09:53:09.815644 4070676 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0606 09:53:09.816263 4070676 gpu_resources.cc:164] device: 2, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
=======================================================================
I0606 09:53:10.296268 4070676 tcp_utils.cc:111] Retry to connect to 127.0.1.1:57799 while the server is not yet listening.
I0606 09:53:13.296458 4070676 tcp_utils.cc:134] Successfully connected to 127.0.1.1:57799
I0606 09:53:13.333060 4070676 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0606 09:53:13.333119 4070676 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0606 09:53:15.658877 4070676 gpu_resources.cc:306] WARNING: device: 2. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
I0606 12:41:35.513321 4070676 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0606 20:16:15.407821 2970356 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0606 20:16:15.408456 2970356 gpu_resources.cc:164] device: 2, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
=======================================================================
I0606 20:16:16.466444 2970356 tcp_utils.cc:111] Retry to connect to 127.0.1.1:61030 while the server is not yet listening.
I0606 20:16:19.466686 2970356 tcp_utils.cc:134] Successfully connected to 127.0.1.1:61030
I0606 20:16:19.496062 2970356 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0606 20:16:19.496094 2970356 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0606 20:16:21.798082 2970356 gpu_resources.cc:306] WARNING: device: 2. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Backward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   StackGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::stack_grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, paddle::Tensor const&, int, std::vector<paddle::Tensor*, std::allocator<paddle::Tensor*> >)
4   phi::KernelImpl<void (*)(phi::GPUContext const&, phi::DenseTensor const&, int, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::StackGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, int, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, phi::DenseTensor const&, int, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::funcs::UnStackRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, int, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >*)
6   void phi::funcs::LaunchUnStackKernel<phi::GPUContext, float, int, (phi::funcs::SegmentedArraySize)4>(phi::GPUContext const&, int, int, int, int, phi::DenseTensor const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >*)

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1749212193 (unix time) try "date -d @1749212193" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0x3e8002d510f) received by PID 2970356 (TID 0x707723268740) from PID 2969871 ***]

/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0606 20:17:28.876168 2988214 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0606 20:17:28.876787 2988214 gpu_resources.cc:164] device: 2, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
=======================================================================
I0606 20:17:29.984906 2988214 tcp_utils.cc:111] Retry to connect to 127.0.1.1:58310 while the server is not yet listening.
I0606 20:17:32.985095 2988214 tcp_utils.cc:134] Successfully connected to 127.0.1.1:58310
I0606 20:17:33.038061 2988214 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0606 20:17:33.038093 2988214 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0606 20:17:35.805707 2988214 gpu_resources.cc:306] WARNING: device: 2. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
After downsampling, x_reshaped shape: [32, 96, 28, 28], H: 28, W: 28
After downsampling, x_reshaped shape: [32, 96, 28, 28], H: 28, W: 28
After downsampling, x_reshaped shape: [32, 192, 14, 14], H: 14, W: 14
After downsampling, x_reshaped shape: [32, 192, 14, 14], H: 14, W: 14
After downsampling, x_reshaped shape: [32, 192, 14, 14], H: 14, W: 14
After downsampling, x_reshaped shape: [32, 192, 14, 14], H: 14, W: 14
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 384, 7, 7], H: 7, W: 7
After downsampling, x_reshaped shape: [32, 768, 3, 3], H: 3, W: 3
After downsampling, x_reshaped shape: [32, 768, 3, 3], H: 3, W: 3


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Backward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   StackGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::stack_grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, paddle::Tensor const&, int, std::vector<paddle::Tensor*, std::allocator<paddle::Tensor*> >)
4   phi::KernelImpl<void (*)(phi::GPUContext const&, phi::DenseTensor const&, int, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::StackGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, int, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, phi::DenseTensor const&, int, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::funcs::UnStackRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, int, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >*)
6   void phi::funcs::LaunchUnStackKernel<phi::GPUContext, float, int, (phi::funcs::SegmentedArraySize)4>(phi::GPUContext const&, int, int, int, int, phi::DenseTensor const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >*)

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1749212268 (unix time) try "date -d @1749212268" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0x3e8002d9709) received by PID 2988214 (TID 0x7fb81b243740) from PID 2987785 ***]

[2025-06-06 20:17:48,165] [ WARNING] dataloader_iter.py:721 - DataLoader 4 workers exit unexpectedly, pids: 2990964, 2990972, 2990980, 2990988
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0606 20:35:10.493882 3248839 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0606 20:35:10.494532 3248839 gpu_resources.cc:164] device: 2, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
=======================================================================
I0606 20:35:10.985723 3248839 tcp_utils.cc:111] Retry to connect to 127.0.1.1:61075 while the server is not yet listening.
I0606 20:35:13.985885 3248839 tcp_utils.cc:134] Successfully connected to 127.0.1.1:61075
I0606 20:35:14.062031 3248839 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0606 20:35:14.062062 3248839 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0606 20:35:16.480971 3248839 gpu_resources.cc:306] WARNING: device: 2. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
feat2.shape after GSA: [16, 56, 56, 160]


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Backward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   Conv2dGradNodeFinal::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::conv2d_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&, paddle::Tensor*, paddle::Tensor*)
4   void phi::ConvCudnnGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&, phi::DenseTensor*, phi::DenseTensor*)
5   std::_Function_handler<void (void*), phi::ConvRunner<float, (phi::ConvKind)2>::Apply(phi::GPUContext const&, phi::ConvArgsBase<cudnnContext*, cudnnDataType_t> const&, phi::SearchResult<cudnnConvolutionBwdDataAlgo_t> const&, float const*, float const*, float*, int, int, int, int, unsigned long, phi::DnnWorkspaceHandle*, bool)::{lambda(void*)#1}>::_M_invoke(std::_Any_data const&, void*&&)

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1749213328 (unix time) try "date -d @1749213328" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0x3e800319136) received by PID 3248839 (TID 0x7db75f9f3740) from PID 3248438 ***]

/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0606 20:36:20.857169 3257162 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0606 20:36:20.857784 3257162 gpu_resources.cc:164] device: 2, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
=======================================================================
I0606 20:36:21.349936 3257162 tcp_utils.cc:134] Successfully connected to 127.0.1.1:37936
I0606 20:36:21.431124 3257162 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0606 20:36:21.431160 3257162 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0606 20:36:23.793707 3257162 gpu_resources.cc:306] WARNING: device: 2. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
W0606 20:45:54.157718 3282725 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0606 20:45:54.158365 3282725 gpu_resources.cc:164] device: 2, cuDNN Version: 8.9.
feat2.shape after GSA: [16, 56, 56, 160]
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
=======================================================================
I0606 20:45:54.677493 3282725 tcp_utils.cc:111] Retry to connect to 127.0.1.1:60980 while the server is not yet listening.
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
I0606 20:45:57.677659 3282725 tcp_utils.cc:134] Successfully connected to 127.0.1.1:60980
I0606 20:45:57.741021 3282725 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0606 20:45:57.741056 3282725 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
feat2.shape after GSA: [16, 56, 56, 160]
W0606 20:46:04.455651 3282725 gpu_resources.cc:306] WARNING: device: 2. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_conv2d(_object*, _object*, _object*)
1   conv2d_ad_func(paddle::Tensor const&, paddle::Tensor const&, std::vector<int, std::allocator<int> >, std::vector<int, std::allocator<int> >, std::string, std::vector<int, std::allocator<int> >, int, std::string)
2   paddle::experimental::conv2d(paddle::Tensor const&, paddle::Tensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&)
3   void phi::ConvCudnnKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&, phi::DenseTensor*)
4   void phi::ConvCudnnKernelImplV7<float, phi::GPUContext>(phi::DenseTensor const*, phi::DenseTensor const*, phi::GPUContext const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, phi::backends::gpu::DataLayout, phi::backends::gpu::DataLayout, bool, bool, int, phi::DenseTensor*)
5   phi::DnnWorkspaceHandle::ReallocWorkspace(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1749213972 (unix time) try "date -d @1749213972" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0x3e8003216c8) received by PID 3282725 (TID 0x713f59cd0740) from PID 3282632 ***]

feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
W0606 20:47:24.970551 3287207 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0606 20:47:24.971236 3287207 gpu_resources.cc:164] device: 2, cuDNN Version: 8.9.
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
=======================================================================
I0606 20:47:25.556818 3287207 tcp_utils.cc:134] Successfully connected to 127.0.1.1:60650
I0606 20:47:25.630019 3287207 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0606 20:47:25.630035 3287207 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
W0606 20:47:29.755481 3287207 gpu_resources.cc:306] WARNING: device: 2. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 55, in <module>
    engine.train()
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 344, in train
    self.train_epoch_func(self, epoch_id, print_batch_step)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 65, in train_epoch
    out = forward(engine, batch)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 117, in forward
    return engine.model(batch[0],depth=batch[1])
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/distributed/parallel.py", line 561, in forward
    outputs = self._layers(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/model_zoo/repvgg.py", line 329, in forward
    out1 = self.stage1(out)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/container.py", line 754, in forward
    input = layer(input)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/model_zoo/repvgg.py", line 175, in forward
    self.se(self.rbr_dense(inputs) + self.rbr_1x1(inputs) + id_out))
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/model_zoo/repvgg.py", line 81, in forward
    y = self.conv(x)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/conv.py", line 742, in forward
    out = F.conv._conv_nd(
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/functional/conv.py", line 150, in _conv_nd
    pre_bias = _C_ops.conv2d(
MemoryError: 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_conv2d(_object*, _object*, _object*)
1   conv2d_ad_func(paddle::Tensor const&, paddle::Tensor const&, std::vector<int, std::allocator<int> >, std::vector<int, std::allocator<int> >, std::string, std::vector<int, std::allocator<int> >, int, std::string)
2   paddle::experimental::conv2d(paddle::Tensor const&, paddle::Tensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&)
3   void phi::ConvCudnnKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&, phi::DenseTensor*)
4   void phi::ConvCudnnKernelImplV7<float, phi::GPUContext>(phi::DenseTensor const*, phi::DenseTensor const*, phi::GPUContext const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, phi::backends::gpu::DataLayout, phi::backends::gpu::DataLayout, bool, bool, int, phi::DenseTensor*)
5   phi::DnnWorkspaceHandle::ReallocWorkspace(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 2. Cannot allocate 141.328125MB memory on GPU 2, 23.571655GB memory has been allocated and available memory is only 72.937500MB.

Please check whether there is any other process using GPU 2.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
I0606 20:47:40.556682 3287207 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0606 20:48:29.063362 3290692 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0606 20:48:29.063963 3290692 gpu_resources.cc:164] device: 2, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
=======================================================================
I0606 20:48:29.475118 3290692 tcp_utils.cc:134] Successfully connected to 127.0.1.1:57553
I0606 20:48:29.545979 3290692 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0606 20:48:29.545994 3290692 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
W0606 20:48:36.498353 3290692 gpu_resources.cc:306] WARNING: device: 2. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_conv2d(_object*, _object*, _object*)
1   conv2d_ad_func(paddle::Tensor const&, paddle::Tensor const&, std::vector<int, std::allocator<int> >, std::vector<int, std::allocator<int> >, std::string, std::vector<int, std::allocator<int> >, int, std::string)
2   paddle::experimental::conv2d(paddle::Tensor const&, paddle::Tensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&)
3   void phi::ConvCudnnKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&, phi::DenseTensor*)
4   void phi::ConvCudnnKernelImplV7<float, phi::GPUContext>(phi::DenseTensor const*, phi::DenseTensor const*, phi::GPUContext const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, phi::backends::gpu::DataLayout, phi::backends::gpu::DataLayout, bool, bool, int, phi::DenseTensor*)
5   phi::DnnWorkspaceHandle::ReallocWorkspace(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1749214124 (unix time) try "date -d @1749214124" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0x3e8003235d7) received by PID 3290692 (TID 0x7090fe45e740) from PID 3290583 ***]

feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
W0606 20:49:55.388803 3294634 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0606 20:49:55.389549 3294634 gpu_resources.cc:164] device: 2, cuDNN Version: 8.9.
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
=======================================================================
I0606 20:49:55.952011 3294634 tcp_utils.cc:111] Retry to connect to 127.0.1.1:53732 while the server is not yet listening.
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
I0606 20:49:58.952204 3294634 tcp_utils.cc:134] Successfully connected to 127.0.1.1:53732
I0606 20:49:58.952421 3294634 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0606 20:49:58.952430 3294634 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 55, in <module>
    engine.train()
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 331, in train
    metric_info = init_model(self.config.Global, self.model,
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/utils/save_load.py", line 130, in init_model
    assert os.path.exists(checkpoints + ".pdparams"), \
AssertionError: Given dir https://paddle-imagenet-models-name.bj.bcebos.com/dygraph/RepVGG_D2se_pretrained.pdparams.pdparams not exist.
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
I0606 20:50:00.049834 3294634 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
W0606 20:50:33.743469 3296514 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0606 20:50:33.744271 3296514 gpu_resources.cc:164] device: 2, cuDNN Version: 8.9.
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
=======================================================================
I0606 20:50:34.301385 3296514 tcp_utils.cc:111] Retry to connect to 127.0.1.1:35733 while the server is not yet listening.
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
I0606 20:50:37.301604 3296514 tcp_utils.cc:134] Successfully connected to 127.0.1.1:35733
feat2.shape after GSA: [4, 80, 80, 160]
I0606 20:50:37.349037 3296514 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0606 20:50:37.349073 3296514 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 55, in <module>
    engine.train()
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 331, in train
    metric_info = init_model(self.config.Global, self.model,
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/utils/save_load.py", line 130, in init_model
    assert os.path.exists(checkpoints + ".pdparams"), \
AssertionError: Given dir https://paddle-imagenet-models-name.bj.bcebos.com/dygraph/RepVGG_D2se_pretrained.pdparams.pdparams not exist.
I0606 20:50:38.424607 3296514 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
W0606 20:51:56.747931 3300083 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0606 20:51:56.748819 3300083 gpu_resources.cc:164] device: 2, cuDNN Version: 8.9.
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
=======================================================================
I0606 20:52:01.013998 3300083 tcp_utils.cc:134] Successfully connected to 127.0.1.1:54280
feat2.shape after GSA: [16, 56, 56, 160]
I0606 20:52:01.045059 3300083 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0606 20:52:01.045082 3300083 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
W0606 20:52:05.454421 3300083 gpu_resources.cc:306] WARNING: device: 2. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 55, in <module>
    engine.train()
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 344, in train
    self.train_epoch_func(self, epoch_id, print_batch_step)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 65, in train_epoch
    out = forward(engine, batch)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 117, in forward
    return engine.model(batch[0],depth=batch[1])
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/distributed/parallel.py", line 561, in forward
    outputs = self._layers(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/model_zoo/repvgg.py", line 339, in forward
    geo1 = self.geo_prior_gen((H1, W1), depth)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/base/theseus_layer.py", line 502, in forward
    mask = self.generate_pos_decay(H, W)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/base/theseus_layer.py", line 448, in generate_pos_decay
    mask = grid.unsqueeze(1) - grid.unsqueeze(0)
MemoryError: 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   subtract_ad_func(paddle::Tensor const&, paddle::Tensor const&)
1   paddle::experimental::subtract(paddle::Tensor const&, paddle::Tensor const&)
2   void phi::SubtractRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*)
3   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
4   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
5   paddle::memory::allocation::Allocator::Allocate(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 2. Cannot allocate 75.031250MB memory on GPU 2, 23.575562GB memory has been allocated and available memory is only 68.937500MB.

Please check whether there is any other process using GPU 2.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

feat2.shape after GSA: [16, 56, 56, 160]
I0606 20:52:16.197425 3300083 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0606 20:54:22.755699 3307138 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0606 20:54:22.756314 3307138 gpu_resources.cc:164] device: 2, cuDNN Version: 8.9.
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
=======================================================================
I0606 20:54:26.771222 3307138 tcp_utils.cc:134] Successfully connected to 127.0.1.1:47225
I0606 20:54:26.820143 3307138 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0606 20:54:26.820190 3307138 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
W0606 20:54:29.130429 3307138 gpu_resources.cc:306] WARNING: device: 2. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 55, in <module>
    engine.train()
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 344, in train
    self.train_epoch_func(self, epoch_id, print_batch_step)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 65, in train_epoch
    out = forward(engine, batch)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 117, in forward
    return engine.model(batch[0],depth=batch[1])
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/distributed/parallel.py", line 561, in forward
    outputs = self._layers(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/model_zoo/repvgg.py", line 339, in forward
    geo1 = self.geo_prior_gen((H1, W1), depth)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/base/theseus_layer.py", line 502, in forward
    mask = self.generate_pos_decay(H, W)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/base/theseus_layer.py", line 448, in generate_pos_decay
    mask = grid.unsqueeze(1) - grid.unsqueeze(0)
MemoryError: 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   subtract_ad_func(paddle::Tensor const&, paddle::Tensor const&)
1   paddle::experimental::subtract(paddle::Tensor const&, paddle::Tensor const&)
2   void phi::SubtractRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*)
3   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
4   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
5   paddle::memory::allocation::Allocator::Allocate(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 2. Cannot allocate 75.031250MB memory on GPU 2, 23.575562GB memory has been allocated and available memory is only 68.937500MB.

Please check whether there is any other process using GPU 2.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

I0606 20:54:39.898550 3307138 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0606 20:59:23.810268 3322154 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0606 20:59:23.810878 3322154 gpu_resources.cc:164] device: 2, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
=======================================================================
I0606 20:59:27.911689 3322154 tcp_utils.cc:134] Successfully connected to 127.0.1.1:60730
I0606 20:59:27.985038 3322154 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0606 20:59:27.985064 3322154 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
feat2.shape after GSA: [16, 56, 56, 160]
W0606 20:59:29.580179 3322154 gpu_resources.cc:306] WARNING: device: 2. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 55, in <module>
    engine.train()
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 344, in train
    self.train_epoch_func(self, epoch_id, print_batch_step)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 65, in train_epoch
    out = forward(engine, batch)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 117, in forward
    return engine.model(batch[0],depth=batch[1])
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/distributed/parallel.py", line 561, in forward
    outputs = self._layers(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/model_zoo/repvgg.py", line 339, in forward
    geo1 = self.geo_prior_gen((H1, W1), depth)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/base/theseus_layer.py", line 504, in forward
    mask = self.weight[0] * mask + self.weight[1] * mask_d
MemoryError: 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   add_ad_func(paddle::Tensor const&, paddle::Tensor const&)
1   paddle::experimental::add(paddle::Tensor const&, paddle::Tensor const&)
2   void phi::AddRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*)
3   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
4   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
5   paddle::memory::allocation::Allocator::Allocate(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 2. Cannot allocate 1.172363GB memory on GPU 2, 23.276733GB memory has been allocated and available memory is only 374.937500MB.

Please check whether there is any other process using GPU 2.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

feat2.shape after GSA: [16, 56, 56, 160]
I0606 20:59:40.455161 3322154 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0606 21:00:05.185066 3324312 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0606 21:00:05.185870 3324312 gpu_resources.cc:164] device: 2, cuDNN Version: 8.9.
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
=======================================================================
I0606 21:00:09.253749 3324312 tcp_utils.cc:134] Successfully connected to 127.0.1.1:61053
I0606 21:00:09.270102 3324312 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0606 21:00:09.270151 3324312 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
W0606 21:00:11.051059 3324312 gpu_resources.cc:306] WARNING: device: 2. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
W0606 21:00:15.420714 3324994 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0606 21:00:15.421481 3324994 gpu_resources.cc:164] device: 2, cuDNN Version: 8.9.
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 32, in <module>
    paddle.device.cuda.empty_cache()
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/device/cuda/__init__.py", line 189, in empty_cache
    core.cuda_empty_cache()
OSError: (External) CUDA error(2), out of memory. 
  [Hint: 'cudaErrorMemoryAllocation'. The API call failed because it was unable to allocate enough memory to perform the requested operation. ] (at ../paddle/phi/backends/gpu/cuda/cuda_info.cc:206)

feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 55, in <module>
    engine.train()
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 344, in train
    self.train_epoch_func(self, epoch_id, print_batch_step)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 65, in train_epoch
    out = forward(engine, batch)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 117, in forward
    return engine.model(batch[0],depth=batch[1])
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/distributed/parallel.py", line 561, in forward
    outputs = self._layers(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/model_zoo/repvgg.py", line 339, in forward
    geo1 = self.geo_prior_gen((H1, W1), depth)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/base/theseus_layer.py", line 504, in forward
    mask = self.weight[0] * mask + self.weight[1] * mask_d
MemoryError: 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   add_ad_func(paddle::Tensor const&, paddle::Tensor const&)
1   paddle::experimental::add(paddle::Tensor const&, paddle::Tensor const&)
2   void phi::AddRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*)
3   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
4   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
5   paddle::memory::allocation::Allocator::Allocate(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 2. Cannot allocate 1.172363GB memory on GPU 2, 23.276733GB memory has been allocated and available memory is only 374.937500MB.

Please check whether there is any other process using GPU 2.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

I0606 21:00:21.713016 3324312 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [16, 56, 56, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [4, 80, 80, 160]
feat2.shape after GSA: [3, 80, 80, 160]
I0606 21:30:24.964402 3257162 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0607 11:01:32.888549 3800429 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0607 11:01:32.889158 3800429 gpu_resources.cc:164] device: 2, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
=======================================================================
I0607 11:01:33.349674 3800429 tcp_utils.cc:134] Successfully connected to 127.0.1.1:57706
I0607 11:01:33.358088 3800429 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0607 11:01:33.358107 3800429 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0607 11:01:35.792264 3800429 gpu_resources.cc:306] WARNING: device: 2. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0607 11:18:11.329033 3837231 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0607 11:18:11.329705 3837231 gpu_resources.cc:164] device: 2, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
=======================================================================
I0607 11:18:11.843521 3837231 tcp_utils.cc:134] Successfully connected to 127.0.1.1:47528
I0607 11:18:11.888026 3837231 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0607 11:18:11.888051 3837231 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   phi::backends::gpu::GpuMemcpySync(void*, void const*, unsigned long, cudaMemcpyKind)

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1749266299 (unix time) try "date -d @1749266299" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0x3e8003a8ce7) received by PID 3837231 (TID 0x7b9961115740) from PID 3837159 ***]

/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0607 11:21:53.698133 3842520 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0607 11:21:53.698913 3842520 gpu_resources.cc:164] device: 2, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
=======================================================================
I0607 11:21:54.191116 3842520 tcp_utils.cc:134] Successfully connected to 127.0.1.1:35213
I0607 11:21:54.225124 3842520 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0607 11:21:54.225165 3842520 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0607 11:21:59.884276 3842520 gpu_resources.cc:306] WARNING: device: 2. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
out1.shape before EMAA: [16, 160, 56, 56]
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 55, in <module>
    engine.train()
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 344, in train
    self.train_epoch_func(self, epoch_id, print_batch_step)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 65, in train_epoch
    out = forward(engine, batch)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 117, in forward
    return engine.model(batch[0],depth=batch[1])
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/distributed/parallel.py", line 561, in forward
    outputs = self._layers(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/model_zoo/repvgg.py", line 354, in forward
    out1 = self.EMAA(out1)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/base/theseus_layer.py", line 684, in forward
    hw = self.conv1x1(paddle.concat([x_h, x_w], axis=2))  # [b*g, c//g, h+w, 1]
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/conv.py", line 742, in forward
    out = F.conv._conv_nd(
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/functional/conv.py", line 150, in _conv_nd
    pre_bias = _C_ops.conv2d(
ValueError: (InvalidArgument) The number of input's channels should be equal to filter's channels * groups for Op(Conv). But received: the input's channels is 160, the input's shape is [16, 160, 112, 1]; the filter's channels is 40, the filter's shape is [40, 40, 1, 1]; the groups is 1, the data_format is NCHW. The error may come from wrong data_format setting.
  [Hint: Expected input_channels == filter_channels * groups, but received input_channels:160 != filter_channels * groups:40.] (at ../paddle/phi/infermeta/binary.cc:651)

I0607 11:22:00.690666 3842520 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0607 11:23:22.896366 3844901 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0607 11:23:22.897197 3844901 gpu_resources.cc:164] device: 2, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
=======================================================================
I0607 11:23:23.326455 3844901 tcp_utils.cc:134] Successfully connected to 127.0.1.1:42966
I0607 11:23:23.355028 3844901 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0607 11:23:23.355049 3844901 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0607 11:23:32.072016 3844901 gpu_resources.cc:306] WARNING: device: 2. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_matmul(_object*, _object*, _object*)
1   matmul_ad_func(paddle::Tensor const&, paddle::Tensor const&, bool, bool)
2   paddle::experimental::matmul(paddle::Tensor const&, paddle::Tensor const&, bool, bool)
3   void phi::MatmulKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, bool, bool, phi::DenseTensor*)
4   void phi::MatMulFunctionImplWithBlas<phi::GPUContext, float>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, std::vector<long, std::allocator<long> > const&, std::vector<long, std::allocator<long> > const&, phi::DenseTensor*, bool, bool, bool, phi::funcs::MatmulPlanner*)
5   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
6   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::RetryAllocator::AllocateImpl(unsigned long)
10  paddle::memory::allocation::StreamSafeCUDAAllocator::AllocateImpl(unsigned long)
11  paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
12  paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)
13  paddle::platform::RecordedGpuFree(void*, unsigned long, int)

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1749266681 (unix time) try "date -d @1749266681" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0x3e8003aaa67) received by PID 3844901 (TID 0x7206437af740) from PID 3844711 ***]

I0607 11:36:46.641433 3800429 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0607 16:32:59.596235 4002659 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0607 16:32:59.596948 4002659 gpu_resources.cc:164] device: 2, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
=======================================================================
I0607 16:33:00.078116 4002659 tcp_utils.cc:134] Successfully connected to 127.0.1.1:63984
I0607 16:33:00.150171 4002659 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0607 16:33:00.150219 4002659 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0607 16:33:05.486306 4002659 gpu_resources.cc:306] WARNING: device: 2. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0607 16:45:57.673385 4047389 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0607 16:45:57.674283 4047389 gpu_resources.cc:164] device: 2, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
=======================================================================
I0607 16:45:59.075119 4047389 tcp_utils.cc:134] Successfully connected to 127.0.1.1:53137
I0607 16:45:59.136106 4047389 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0607 16:45:59.136143 4047389 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0607 16:46:01.005712 4047389 gpu_resources.cc:306] WARNING: device: 2. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 55, in <module>
    engine.train()
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 344, in train
    self.train_epoch_func(self, epoch_id, print_batch_step)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 65, in train_epoch
    out = forward(engine, batch)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 117, in forward
    return engine.model(batch[0],depth=batch[1])
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/distributed/parallel.py", line 561, in forward
    outputs = self._layers(*inputs, **kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/model_zoo/repvgg.py", line 343, in forward
    geo1 = self.geo_prior_gen((H1, W1), depth)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/nn/layer/layers.py", line 1567, in __call__
    return self.forward(*inputs, **kwargs)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/base/theseus_layer.py", line 502, in forward
    mask = self.generate_pos_decay(H, W)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/arch/backbone/base/theseus_layer.py", line 448, in generate_pos_decay
    mask = grid.unsqueeze(1) - grid.unsqueeze(0)
MemoryError: 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   subtract_ad_func(paddle::Tensor const&, paddle::Tensor const&)
1   paddle::experimental::subtract(paddle::Tensor const&, paddle::Tensor const&)
2   void phi::SubtractRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*)
3   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
4   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
5   paddle::memory::allocation::Allocator::Allocate(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 2. Cannot allocate 75.031250MB memory on GPU 2, 23.593140GB memory has been allocated and available memory is only 50.937500MB.

Please check whether there is any other process using GPU 2.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

I0607 16:46:11.791462 4047389 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0607 16:53:57.662117 4057563 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0607 16:53:57.662817 4057563 gpu_resources.cc:164] device: 2, cuDNN Version: 8.9.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1749286439 (unix time) try "date -d @1749286439" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0x3e8003de98b) received by PID 4057563 (TID 0x714880c74740) from PID 4057483 ***]

/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0607 16:55:19.938946 4058406 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0607 16:55:19.939612 4058406 gpu_resources.cc:164] device: 2, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
=======================================================================
I0607 16:55:21.103905 4058406 tcp_utils.cc:134] Successfully connected to 127.0.1.1:43304
I0607 16:55:21.136089 4058406 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0607 16:55:21.136112 4058406 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0607 16:55:22.563297 4058406 gpu_resources.cc:306] WARNING: device: 2. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   void phi::funcs::set_constant_with_place<phi::GPUPlace>(phi::DeviceContext const&, phi::DenseTensor*, float)
1   phi::funcs::SetConstant<phi::GPUContext, float>::operator()(phi::GPUContext const&, phi::DenseTensor*, float)

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1749290350 (unix time) try "date -d @1749290350" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0x3e800011ebc) received by PID 4058406 (TID 0x71fbb7a67740) from PID 73404 ***]

/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0607 18:00:21.728111 74145 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0607 18:00:21.728731 74145 gpu_resources.cc:164] device: 2, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
=======================================================================
I0607 18:00:22.919793 74145 tcp_utils.cc:111] Retry to connect to 127.0.1.1:44953 while the server is not yet listening.
I0607 18:00:25.919981 74145 tcp_utils.cc:134] Successfully connected to 127.0.1.1:44953
I0607 18:00:25.982163 74145 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0607 18:00:25.982206 74145 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0607 18:00:27.414173 74145 gpu_resources.cc:306] WARNING: device: 2. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   void phi::funcs::set_constant_with_place<phi::GPUPlace>(phi::DeviceContext const&, phi::DenseTensor*, float)
1   phi::funcs::SetConstant<phi::GPUContext, float>::operator()(phi::GPUContext const&, phi::DenseTensor*, float)

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1749293469 (unix time) try "date -d @1749293469" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0x3e80003dfba) received by PID 74145 (TID 0x73d96958c740) from PID 253882 ***]

/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0607 18:56:40.708894 256561 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0607 18:56:40.709534 256561 gpu_resources.cc:164] device: 2, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
=======================================================================
I0607 18:56:41.919673 256561 tcp_utils.cc:134] Successfully connected to 127.0.1.1:57399
I0607 18:56:41.937176 256561 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0607 18:56:41.937213 256561 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0607 18:56:43.469767 256561 gpu_resources.cc:306] WARNING: device: 2. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
I0607 23:59:32.559520 256561 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0613 18:33:34.811295 248552 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0613 18:33:34.813861 248552 gpu_resources.cc:164] device: 2, cuDNN Version: 8.9.
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 54, in <module>
    engine = Engine(config, mode="train")
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 139, in __init__
    self.train_dataloader = build_dataloader(
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/data/__init__.py", line 116, in build_dataloader
    dataset = eval(dataset_name)(**config_dataset)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/data/dataloader/imagenet_dataset.py", line 45, in __init__
    super(ImageNetDataset, self).__init__(image_root, cls_label_path,
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/data/dataloader/common_dataset.py", line 63, in __init__
    self._load_anno()
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/data/dataloader/imagenet_dataset.py", line 85, in _load_anno
    assert os.path.exists(depth_path), f"depth path {depth_path} does not exist."
AssertionError: depth path /mnt/data1_hdd/wgk/PaddleClas/tr/datasetsmuti/train/4/(8)_disp_disp.png does not exist.
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0613 18:41:26.871172 253903 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0613 18:41:26.871716 253903 gpu_resources.cc:164] device: 2, cuDNN Version: 8.9.
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 54, in <module>
    engine = Engine(config, mode="train")
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 139, in __init__
    self.train_dataloader = build_dataloader(
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/data/__init__.py", line 116, in build_dataloader
    dataset = eval(dataset_name)(**config_dataset)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/data/dataloader/imagenet_dataset.py", line 45, in __init__
    super(ImageNetDataset, self).__init__(image_root, cls_label_path,
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/data/dataloader/common_dataset.py", line 63, in __init__
    self._load_anno()
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/data/dataloader/imagenet_dataset.py", line 79, in _load_anno
    assert os.path.exists(self.images[
AssertionError: path /mnt/data1_hdd/wgk/PaddleClas/tr/datasetsmuti/train/4/(7).jpg does not exist.
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0613 18:50:24.936273 261831 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0613 18:50:24.936914 261831 gpu_resources.cc:164] device: 2, cuDNN Version: 8.9.
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 54, in <module>
    engine = Engine(config, mode="train")
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 213, in __init__
    self.train_metric_func = build_metrics(metric_config)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/metric/__init__.py", line 75, in build_metrics
    metrics_list = CombinedMetrics(copy.deepcopy(config))
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/metric/__init__.py", line 46, in __init__
    eval(metric_name)(**metric_params))
  File "<string>", line 1, in <module>
NameError: name 'Auc' is not defined
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0613 18:53:03.111593 263685 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0613 18:53:03.112221 263685 gpu_resources.cc:164] device: 2, cuDNN Version: 8.9.
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 54, in <module>
    engine = Engine(config, mode="train")
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 213, in __init__
    self.train_metric_func = build_metrics(metric_config)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/metric/__init__.py", line 75, in build_metrics
    metrics_list = CombinedMetrics(copy.deepcopy(config))
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/metric/__init__.py", line 46, in __init__
    eval(metric_name)(**metric_params))
TypeError: __init__() got an unexpected keyword argument 'num_classes'
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0613 18:58:14.223327 267207 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0613 18:58:14.223855 267207 gpu_resources.cc:164] device: 2, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
=======================================================================
I0613 18:58:15.562110 267207 tcp_utils.cc:111] Retry to connect to 127.0.1.1:45901 while the server is not yet listening.
I0613 18:58:18.562285 267207 tcp_utils.cc:134] Successfully connected to 127.0.1.1:45901
I0613 18:58:18.604126 267207 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0613 18:58:18.604163 267207 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0613 18:58:20.356220 267207 gpu_resources.cc:306] WARNING: device: 2. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Backward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   Conv2dGradNodeFinal::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::conv2d_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&, paddle::Tensor*, paddle::Tensor*)
4   void phi::ConvCudnnGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&, phi::DenseTensor*, phi::DenseTensor*)
5   phi::SearchResult<cudnnConvolutionBwdDataAlgo_t> phi::SearchAlgorithm<(phi::ConvKind)2>::Find<float>(phi::GPUContext const&, phi::ConvArgsBase<cudnnContext*, cudnnDataType_t> const&, bool, bool, bool)

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1749812312 (unix time) try "date -d @1749812312" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0x3e800041386) received by PID 267207 (TID 0x75ce59977740) from PID 267142 ***]

/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0613 19:11:33.057546 277255 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0613 19:11:33.058195 277255 gpu_resources.cc:164] device: 2, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_cudnn_batchnorm_spatial_persistent', current_value=True, default_value=False)
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_max_inplace_grad_add', current_value=8, default_value=0)
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
=======================================================================
I0613 19:11:33.425999 277255 tcp_utils.cc:111] Retry to connect to 127.0.1.1:55550 while the server is not yet listening.
I0613 19:11:36.426177 277255 tcp_utils.cc:134] Successfully connected to 127.0.1.1:55550
I0613 19:11:36.455060 277255 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0613 19:11:36.455097 277255 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
Process Process-10:
Traceback (most recent call last):
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 401, in _worker_loop
    tensor_list = [
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 403, in <listcomp>
    numpy2lodtensor(b)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 398, in numpy2lodtensor
    lodtensor.set(arr, core.CPUPlace())
ValueError: (InvalidArgument) Input object type error or incompatible array data type. tensor.set() supports array with bool, float16, float32, float64, int8, int16, int32, int64, uint8 or uint16, please check your input or input array data type. (at ../paddle/fluid/pybind/tensor_py.h:565)

Process Process-6:
Traceback (most recent call last):
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 401, in _worker_loop
    tensor_list = [
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 403, in <listcomp>
    numpy2lodtensor(b)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 398, in numpy2lodtensor
    lodtensor.set(arr, core.CPUPlace())
ValueError: (InvalidArgument) Input object type error or incompatible array data type. tensor.set() supports array with bool, float16, float32, float64, int8, int16, int32, int64, uint8 or uint16, please check your input or input array data type. (at ../paddle/fluid/pybind/tensor_py.h:565)

Process Process-4:
Traceback (most recent call last):
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 401, in _worker_loop
    tensor_list = [
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 403, in <listcomp>
    numpy2lodtensor(b)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 398, in numpy2lodtensor
    lodtensor.set(arr, core.CPUPlace())
ValueError: (InvalidArgument) Input object type error or incompatible array data type. tensor.set() supports array with bool, float16, float32, float64, int8, int16, int32, int64, uint8 or uint16, please check your input or input array data type. (at ../paddle/fluid/pybind/tensor_py.h:565)

Process Process-9:
Traceback (most recent call last):
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 401, in _worker_loop
    tensor_list = [
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 403, in <listcomp>
    numpy2lodtensor(b)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 398, in numpy2lodtensor
    lodtensor.set(arr, core.CPUPlace())
ValueError: (InvalidArgument) Input object type error or incompatible array data type. tensor.set() supports array with bool, float16, float32, float64, int8, int16, int32, int64, uint8 or uint16, please check your input or input array data type. (at ../paddle/fluid/pybind/tensor_py.h:565)

Process Process-11:
Traceback (most recent call last):
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 401, in _worker_loop
    tensor_list = [
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 403, in <listcomp>
    numpy2lodtensor(b)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 398, in numpy2lodtensor
    lodtensor.set(arr, core.CPUPlace())
ValueError: (InvalidArgument) Input object type error or incompatible array data type. tensor.set() supports array with bool, float16, float32, float64, int8, int16, int32, int64, uint8 or uint16, please check your input or input array data type. (at ../paddle/fluid/pybind/tensor_py.h:565)

Process Process-3:
Traceback (most recent call last):
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 401, in _worker_loop
    tensor_list = [
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 403, in <listcomp>
    numpy2lodtensor(b)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 398, in numpy2lodtensor
    lodtensor.set(arr, core.CPUPlace())
ValueError: (InvalidArgument) Input object type error or incompatible array data type. tensor.set() supports array with bool, float16, float32, float64, int8, int16, int32, int64, uint8 or uint16, please check your input or input array data type. (at ../paddle/fluid/pybind/tensor_py.h:565)

Process Process-2:
Traceback (most recent call last):
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 401, in _worker_loop
    tensor_list = [
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 403, in <listcomp>
    numpy2lodtensor(b)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 398, in numpy2lodtensor
    lodtensor.set(arr, core.CPUPlace())
ValueError: (InvalidArgument) Input object type error or incompatible array data type. tensor.set() supports array with bool, float16, float32, float64, int8, int16, int32, int64, uint8 or uint16, please check your input or input array data type. (at ../paddle/fluid/pybind/tensor_py.h:565)

Process Process-5:
Traceback (most recent call last):
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 401, in _worker_loop
    tensor_list = [
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 403, in <listcomp>
    numpy2lodtensor(b)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 398, in numpy2lodtensor
    lodtensor.set(arr, core.CPUPlace())
ValueError: (InvalidArgument) Input object type error or incompatible array data type. tensor.set() supports array with bool, float16, float32, float64, int8, int16, int32, int64, uint8 or uint16, please check your input or input array data type. (at ../paddle/fluid/pybind/tensor_py.h:565)

Process Process-1:
Traceback (most recent call last):
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 401, in _worker_loop
    tensor_list = [
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 403, in <listcomp>
    numpy2lodtensor(b)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 398, in numpy2lodtensor
    lodtensor.set(arr, core.CPUPlace())
ValueError: (InvalidArgument) Input object type error or incompatible array data type. tensor.set() supports array with bool, float16, float32, float64, int8, int16, int32, int64, uint8 or uint16, please check your input or input array data type. (at ../paddle/fluid/pybind/tensor_py.h:565)

Process Process-19:
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 31, in train_epoch
    batch = next(engine.train_dataloader_iter)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/dataloader_iter.py", line 840, in __next__
    self._reader.read_next_list()[0]
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/multiprocess_utils.py", line 133, in __handler__
    core._throw_error_if_process_failed()
SystemError: (Fatal) DataLoader process (pid 277444) exited unexpectedly with code 1. Error detailed are lost due to multiprocessing. Rerunning with:
  1. If run DataLoader by DataLoader.from_generator(...), run with DataLoader.from_generator(..., use_multiprocess=False) may give better error trace.
  2. If run DataLoader by DataLoader(dataset, ...), run with DataLoader(dataset, ..., num_workers=0) may give better error trace (at ../paddle/fluid/imperative/data_loader.cc:150)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 401, in _worker_loop
    tensor_list = [
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 403, in <listcomp>
    numpy2lodtensor(b)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 398, in numpy2lodtensor
    lodtensor.set(arr, core.CPUPlace())
ValueError: (InvalidArgument) Input object type error or incompatible array data type. tensor.set() supports array with bool, float16, float32, float64, int8, int16, int32, int64, uint8 or uint16, please check your input or input array data type. (at ../paddle/fluid/pybind/tensor_py.h:565)

Process Process-24:
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 31, in train_epoch
    batch = next(engine.train_dataloader_iter)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/dataloader_iter.py", line 840, in __next__
    self._reader.read_next_list()[0]
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/multiprocess_utils.py", line 133, in __handler__
    core._throw_error_if_process_failed()
SystemError: (Fatal) DataLoader process (pid 277444) exited unexpectedly with code 1. Error detailed are lost due to multiprocessing. Rerunning with:
  1. If run DataLoader by DataLoader.from_generator(...), run with DataLoader.from_generator(..., use_multiprocess=False) may give better error trace.
  2. If run DataLoader by DataLoader(dataset, ...), run with DataLoader(dataset, ..., num_workers=0) may give better error trace (at ../paddle/fluid/imperative/data_loader.cc:150)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 401, in _worker_loop
    tensor_list = [
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 403, in <listcomp>
    numpy2lodtensor(b)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 398, in numpy2lodtensor
    lodtensor.set(arr, core.CPUPlace())
ValueError: (InvalidArgument) Input object type error or incompatible array data type. tensor.set() supports array with bool, float16, float32, float64, int8, int16, int32, int64, uint8 or uint16, please check your input or input array data type. (at ../paddle/fluid/pybind/tensor_py.h:565)

Process Process-17:
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 31, in train_epoch
    batch = next(engine.train_dataloader_iter)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/dataloader_iter.py", line 840, in __next__
    self._reader.read_next_list()[0]
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/multiprocess_utils.py", line 133, in __handler__
    core._throw_error_if_process_failed()
SystemError: (Fatal) DataLoader process (pid 277444) exited unexpectedly with code 1. Error detailed are lost due to multiprocessing. Rerunning with:
  1. If run DataLoader by DataLoader.from_generator(...), run with DataLoader.from_generator(..., use_multiprocess=False) may give better error trace.
  2. If run DataLoader by DataLoader(dataset, ...), run with DataLoader(dataset, ..., num_workers=0) may give better error trace (at ../paddle/fluid/imperative/data_loader.cc:150)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 401, in _worker_loop
    tensor_list = [
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 403, in <listcomp>
    numpy2lodtensor(b)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 398, in numpy2lodtensor
    lodtensor.set(arr, core.CPUPlace())
ValueError: (InvalidArgument) Input object type error or incompatible array data type. tensor.set() supports array with bool, float16, float32, float64, int8, int16, int32, int64, uint8 or uint16, please check your input or input array data type. (at ../paddle/fluid/pybind/tensor_py.h:565)

Process Process-13:
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 31, in train_epoch
    batch = next(engine.train_dataloader_iter)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/dataloader_iter.py", line 840, in __next__
    self._reader.read_next_list()[0]
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/multiprocess_utils.py", line 133, in __handler__
    core._throw_error_if_process_failed()
SystemError: (Fatal) DataLoader process (pid 277444) exited unexpectedly with code 1. Error detailed are lost due to multiprocessing. Rerunning with:
  1. If run DataLoader by DataLoader.from_generator(...), run with DataLoader.from_generator(..., use_multiprocess=False) may give better error trace.
  2. If run DataLoader by DataLoader(dataset, ...), run with DataLoader(dataset, ..., num_workers=0) may give better error trace (at ../paddle/fluid/imperative/data_loader.cc:150)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 401, in _worker_loop
    tensor_list = [
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 403, in <listcomp>
    numpy2lodtensor(b)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 398, in numpy2lodtensor
    lodtensor.set(arr, core.CPUPlace())
ValueError: (InvalidArgument) Input object type error or incompatible array data type. tensor.set() supports array with bool, float16, float32, float64, int8, int16, int32, int64, uint8 or uint16, please check your input or input array data type. (at ../paddle/fluid/pybind/tensor_py.h:565)

Process Process-20:
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 31, in train_epoch
    batch = next(engine.train_dataloader_iter)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/dataloader_iter.py", line 840, in __next__
    self._reader.read_next_list()[0]
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/multiprocess_utils.py", line 133, in __handler__
    core._throw_error_if_process_failed()
SystemError: (Fatal) DataLoader process (pid 277444) exited unexpectedly with code 1. Error detailed are lost due to multiprocessing. Rerunning with:
  1. If run DataLoader by DataLoader.from_generator(...), run with DataLoader.from_generator(..., use_multiprocess=False) may give better error trace.
  2. If run DataLoader by DataLoader(dataset, ...), run with DataLoader(dataset, ..., num_workers=0) may give better error trace (at ../paddle/fluid/imperative/data_loader.cc:150)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 401, in _worker_loop
    tensor_list = [
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 403, in <listcomp>
    numpy2lodtensor(b)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 398, in numpy2lodtensor
    lodtensor.set(arr, core.CPUPlace())
ValueError: (InvalidArgument) Input object type error or incompatible array data type. tensor.set() supports array with bool, float16, float32, float64, int8, int16, int32, int64, uint8 or uint16, please check your input or input array data type. (at ../paddle/fluid/pybind/tensor_py.h:565)

Process Process-16:
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 31, in train_epoch
    batch = next(engine.train_dataloader_iter)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/dataloader_iter.py", line 840, in __next__
    self._reader.read_next_list()[0]
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/multiprocess_utils.py", line 133, in __handler__
    core._throw_error_if_process_failed()
SystemError: (Fatal) DataLoader process (pid 277444) exited unexpectedly with code 1. Error detailed are lost due to multiprocessing. Rerunning with:
  1. If run DataLoader by DataLoader.from_generator(...), run with DataLoader.from_generator(..., use_multiprocess=False) may give better error trace.
  2. If run DataLoader by DataLoader(dataset, ...), run with DataLoader(dataset, ..., num_workers=0) may give better error trace (at ../paddle/fluid/imperative/data_loader.cc:150)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 401, in _worker_loop
    tensor_list = [
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 403, in <listcomp>
    numpy2lodtensor(b)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 398, in numpy2lodtensor
    lodtensor.set(arr, core.CPUPlace())
ValueError: (InvalidArgument) Input object type error or incompatible array data type. tensor.set() supports array with bool, float16, float32, float64, int8, int16, int32, int64, uint8 or uint16, please check your input or input array data type. (at ../paddle/fluid/pybind/tensor_py.h:565)

Process Process-22:
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 31, in train_epoch
    batch = next(engine.train_dataloader_iter)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/dataloader_iter.py", line 840, in __next__
    self._reader.read_next_list()[0]
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/multiprocess_utils.py", line 133, in __handler__
    core._throw_error_if_process_failed()
SystemError: (Fatal) DataLoader process (pid 277444) exited unexpectedly with code 1. Error detailed are lost due to multiprocessing. Rerunning with:
  1. If run DataLoader by DataLoader.from_generator(...), run with DataLoader.from_generator(..., use_multiprocess=False) may give better error trace.
  2. If run DataLoader by DataLoader(dataset, ...), run with DataLoader(dataset, ..., num_workers=0) may give better error trace (at ../paddle/fluid/imperative/data_loader.cc:150)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 401, in _worker_loop
    tensor_list = [
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 403, in <listcomp>
    numpy2lodtensor(b)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 398, in numpy2lodtensor
    lodtensor.set(arr, core.CPUPlace())
ValueError: (InvalidArgument) Input object type error or incompatible array data type. tensor.set() supports array with bool, float16, float32, float64, int8, int16, int32, int64, uint8 or uint16, please check your input or input array data type. (at ../paddle/fluid/pybind/tensor_py.h:565)

Process Process-23:
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 31, in train_epoch
    batch = next(engine.train_dataloader_iter)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/dataloader_iter.py", line 840, in __next__
    self._reader.read_next_list()[0]
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/multiprocess_utils.py", line 133, in __handler__
    core._throw_error_if_process_failed()
SystemError: (Fatal) DataLoader process (pid 277444) exited unexpectedly with code 1. Error detailed are lost due to multiprocessing. Rerunning with:
  1. If run DataLoader by DataLoader.from_generator(...), run with DataLoader.from_generator(..., use_multiprocess=False) may give better error trace.
  2. If run DataLoader by DataLoader(dataset, ...), run with DataLoader(dataset, ..., num_workers=0) may give better error trace (at ../paddle/fluid/imperative/data_loader.cc:150)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 401, in _worker_loop
    tensor_list = [
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 403, in <listcomp>
    numpy2lodtensor(b)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 398, in numpy2lodtensor
    lodtensor.set(arr, core.CPUPlace())
ValueError: (InvalidArgument) Input object type error or incompatible array data type. tensor.set() supports array with bool, float16, float32, float64, int8, int16, int32, int64, uint8 or uint16, please check your input or input array data type. (at ../paddle/fluid/pybind/tensor_py.h:565)

Process Process-18:
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 31, in train_epoch
    batch = next(engine.train_dataloader_iter)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/dataloader_iter.py", line 840, in __next__
    self._reader.read_next_list()[0]
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/multiprocess_utils.py", line 133, in __handler__
    core._throw_error_if_process_failed()
SystemError: (Fatal) DataLoader process (pid 277444) exited unexpectedly with code 1. Error detailed are lost due to multiprocessing. Rerunning with:
  1. If run DataLoader by DataLoader.from_generator(...), run with DataLoader.from_generator(..., use_multiprocess=False) may give better error trace.
  2. If run DataLoader by DataLoader(dataset, ...), run with DataLoader(dataset, ..., num_workers=0) may give better error trace (at ../paddle/fluid/imperative/data_loader.cc:150)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 401, in _worker_loop
    tensor_list = [
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 403, in <listcomp>
    numpy2lodtensor(b)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 398, in numpy2lodtensor
    lodtensor.set(arr, core.CPUPlace())
ValueError: (InvalidArgument) Input object type error or incompatible array data type. tensor.set() supports array with bool, float16, float32, float64, int8, int16, int32, int64, uint8 or uint16, please check your input or input array data type. (at ../paddle/fluid/pybind/tensor_py.h:565)

Process Process-15:
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 31, in train_epoch
    batch = next(engine.train_dataloader_iter)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/dataloader_iter.py", line 840, in __next__
    self._reader.read_next_list()[0]
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/multiprocess_utils.py", line 133, in __handler__
    core._throw_error_if_process_failed()
SystemError: (Fatal) DataLoader process (pid 277444) exited unexpectedly with code 1. Error detailed are lost due to multiprocessing. Rerunning with:
  1. If run DataLoader by DataLoader.from_generator(...), run with DataLoader.from_generator(..., use_multiprocess=False) may give better error trace.
  2. If run DataLoader by DataLoader(dataset, ...), run with DataLoader(dataset, ..., num_workers=0) may give better error trace (at ../paddle/fluid/imperative/data_loader.cc:150)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 401, in _worker_loop
    tensor_list = [
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 403, in <listcomp>
    numpy2lodtensor(b)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 398, in numpy2lodtensor
    lodtensor.set(arr, core.CPUPlace())
ValueError: (InvalidArgument) Input object type error or incompatible array data type. tensor.set() supports array with bool, float16, float32, float64, int8, int16, int32, int64, uint8 or uint16, please check your input or input array data type. (at ../paddle/fluid/pybind/tensor_py.h:565)

Process Process-21:
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 31, in train_epoch
    batch = next(engine.train_dataloader_iter)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/dataloader_iter.py", line 840, in __next__
    self._reader.read_next_list()[0]
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/multiprocess_utils.py", line 133, in __handler__
    core._throw_error_if_process_failed()
SystemError: (Fatal) DataLoader process (pid 277444) exited unexpectedly with code 1. Error detailed are lost due to multiprocessing. Rerunning with:
  1. If run DataLoader by DataLoader.from_generator(...), run with DataLoader.from_generator(..., use_multiprocess=False) may give better error trace.
  2. If run DataLoader by DataLoader(dataset, ...), run with DataLoader(dataset, ..., num_workers=0) may give better error trace (at ../paddle/fluid/imperative/data_loader.cc:150)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 401, in _worker_loop
    tensor_list = [
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 403, in <listcomp>
    numpy2lodtensor(b)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/worker.py", line 398, in numpy2lodtensor
    lodtensor.set(arr, core.CPUPlace())
ValueError: (InvalidArgument) Input object type error or incompatible array data type. tensor.set() supports array with bool, float16, float32, float64, int8, int16, int32, int64, uint8 or uint16, please check your input or input array data type. (at ../paddle/fluid/pybind/tensor_py.h:565)

[2025-06-13 19:11:45,160] [ WARNING] dataloader_iter.py:721 - DataLoader 9 workers exit unexpectedly, pids: 277428, 277437, 277444, 277454, 277462, 277474, 277494, 277502, 277510
[2025-06-13 19:11:46,141] [ WARNING] dataloader_iter.py:721 - DataLoader 11 workers exit unexpectedly, pids: 277679, 277683, 277685, 277687, 277689, 277691, 277693, 277695, 277697, 277699, 277701
Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 31, in train_epoch
    batch = next(engine.train_dataloader_iter)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/dataloader_iter.py", line 840, in __next__
    self._reader.read_next_list()[0]
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/multiprocess_utils.py", line 133, in __handler__
    core._throw_error_if_process_failed()
SystemError: (Fatal) DataLoader process (pid 277444) exited unexpectedly with code 1. Error detailed are lost due to multiprocessing. Rerunning with:
  1. If run DataLoader by DataLoader.from_generator(...), run with DataLoader.from_generator(..., use_multiprocess=False) may give better error trace.
  2. If run DataLoader by DataLoader(dataset, ...), run with DataLoader(dataset, ..., num_workers=0) may give better error trace (at ../paddle/fluid/imperative/data_loader.cc:150)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/data1_hdd/wgk/PaddleClas/tools/train.py", line 55, in <module>
    engine.train()
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/engine.py", line 344, in train
    self.train_epoch_func(self, epoch_id, print_batch_step)
  File "/mnt/data1_hdd/wgk/PaddleClas/ppcls/engine/train/train.py", line 37, in train_epoch
    batch = next(engine.train_dataloader_iter)
  File "/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/io/dataloader/dataloader_iter.py", line 840, in __next__
    self._reader.read_next_list()[0]
SystemError: (Fatal) Blocking queue is killed because the data reader raises an exception.
  [Hint: Expected killed_ != true, but received killed_:1 == true:1.] (at ../paddle/phi/core/operators/reader/blocking_queue.h:175)

I0613 19:11:46.454743 277255 process_group_nccl.cc:159] ProcessGroupNCCL destruct 
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0614 22:30:47.105439 2594438 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0614 22:30:47.106133 2594438 gpu_resources.cc:164] device: 2, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
=======================================================================
I0614 22:30:48.324954 2594438 tcp_utils.cc:111] Retry to connect to 127.0.1.1:56710 while the server is not yet listening.
I0614 22:30:51.325127 2594438 tcp_utils.cc:134] Successfully connected to 127.0.1.1:56710
I0614 22:30:51.345058 2594438 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0614 22:30:51.345075 2594438 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0614 22:30:53.026047 2594438 gpu_resources.cc:306] WARNING: device: 2. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
out1.shape before EMAA: [16, 160, 56, 56]


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Backward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   Conv2dGradNodeFinal::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::conv2d_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&, paddle::Tensor*, paddle::Tensor*)
4   void phi::ConvCudnnGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&, phi::DenseTensor*, phi::DenseTensor*)
5   std::_Function_handler<void (void*), phi::ConvRunner<float, (phi::ConvKind)2>::Apply(phi::GPUContext const&, phi::ConvArgsBase<cudnnContext*, cudnnDataType_t> const&, phi::SearchResult<cudnnConvolutionBwdDataAlgo_t> const&, float const*, float const*, float*, int, int, int, int, unsigned long, phi::DnnWorkspaceHandle*, bool)::{lambda(void*)#1}>::_M_invoke(std::_Any_data const&, void*&&)

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1749911464 (unix time) try "date -d @1749911464" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0x3e80027963d) received by PID 2594438 (TID 0x752f077df740) from PID 2594365 ***]

/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0615 21:20:12.650465 685852 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0615 21:20:12.651119 685852 gpu_resources.cc:164] device: 2, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
=======================================================================
I0615 21:20:13.921294 685852 tcp_utils.cc:111] Retry to connect to 127.0.1.1:55200 while the server is not yet listening.
I0615 21:20:16.921523 685852 tcp_utils.cc:134] Successfully connected to 127.0.1.1:55200
I0615 21:20:16.960223 685852 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0615 21:20:16.960258 685852 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0615 21:20:18.872427 685852 gpu_resources.cc:306] WARNING: device: 2. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Backward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   Conv2dGradNodeFinal::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::conv2d_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&, paddle::Tensor*, paddle::Tensor*)
4   void phi::ConvCudnnGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&, phi::DenseTensor*, phi::DenseTensor*)
5   std::_Function_handler<void (void*), phi::ConvRunner<float, (phi::ConvKind)2>::Apply(phi::GPUContext const&, phi::ConvArgsBase<cudnnContext*, cudnnDataType_t> const&, phi::SearchResult<cudnnConvolutionBwdDataAlgo_t> const&, float const*, float const*, float*, int, int, int, int, unsigned long, phi::DnnWorkspaceHandle*, bool)::{lambda(void*)#1}>::_M_invoke(std::_Any_data const&, void*&&)

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1749993630 (unix time) try "date -d @1749993630" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0x3e8000a76c6) received by PID 685852 (TID 0x782cd5ee1740) from PID 685766 ***]

/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0615 21:22:09.124750 689046 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0615 21:22:09.125391 689046 gpu_resources.cc:164] device: 2, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
=======================================================================
I0615 21:22:10.408042 689046 tcp_utils.cc:134] Successfully connected to 127.0.1.1:52364
I0615 21:22:10.464159 689046 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0615 21:22:10.464193 689046 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0615 21:22:12.421636 689046 gpu_resources.cc:306] WARNING: device: 2. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Backward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   Conv2dGradNodeFinal::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::conv2d_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&, paddle::Tensor*, paddle::Tensor*)
4   void phi::ConvCudnnGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&, phi::DenseTensor*, phi::DenseTensor*)
5   std::_Function_handler<void (void*), phi::ConvRunner<float, (phi::ConvKind)2>::Apply(phi::GPUContext const&, phi::ConvArgsBase<cudnnContext*, cudnnDataType_t> const&, phi::SearchResult<cudnnConvolutionBwdDataAlgo_t> const&, float const*, float const*, float*, int, int, int, int, unsigned long, phi::DnnWorkspaceHandle*, bool)::{lambda(void*)#1}>::_M_invoke(std::_Any_data const&, void*&&)

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1749993743 (unix time) try "date -d @1749993743" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0x3e8000a833e) received by PID 689046 (TID 0x7c3eec496740) from PID 688958 ***]

[2025-06-15 21:22:23,895] [ WARNING] dataloader_iter.py:721 - DataLoader 4 workers exit unexpectedly, pids: 689235, 689239, 689247, 689255
/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md
  warnings.warn(warning_message)
W0615 21:22:35.617182 690058 gpu_resources.cc:119] Please NOTE: device: 2, GPU Compute Capability: 8.9, Driver API Version: 12.4, Runtime API Version: 11.8
W0615 21:22:35.617759 690058 gpu_resources.cc:164] device: 2, cuDNN Version: 8.9.
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_cusparse_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusparse/lib', default_value='')
FLAGS(name='FLAGS_selected_gpus', current_value='2', default_value='')
FLAGS(name='FLAGS_flagcx_dir', current_value='/build/lib', default_value='')
FLAGS(name='FLAGS_cublas_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cublas/lib', default_value='')
FLAGS(name='FLAGS_nvidia_package_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia', default_value='')
FLAGS(name='FLAGS_curand_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/curand/lib', default_value='')
FLAGS(name='FLAGS_nccl_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/nccl/lib', default_value='')
FLAGS(name='FLAGS_cupti_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cuda_cupti/lib', default_value='')
FLAGS(name='FLAGS_enable_pir_in_executor', current_value=True, default_value=False)
FLAGS(name='FLAGS_cudnn_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cudnn/lib', default_value='')
FLAGS(name='FLAGS_cusolver_dir', current_value='/home/ren6/miniconda3/envs/dinov2/lib/python3.9/site-packages/paddle/../nvidia/cusolver/lib', default_value='')
=======================================================================
I0615 21:22:36.896836 690058 tcp_utils.cc:134] Successfully connected to 127.0.1.1:50940
I0615 21:22:36.933005 690058 process_group_nccl.cc:151] ProcessGroupNCCL pg_timeout_ 1800000
I0615 21:22:36.933025 690058 process_group_nccl.cc:152] ProcessGroupNCCL nccl_comm_init_option_ 0
W0615 21:22:41.997409 690058 gpu_resources.cc:306] WARNING: device: 2. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.9, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Backward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   Conv2dGradNodeFinal::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::conv2d_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&, paddle::Tensor*, paddle::Tensor*)
4   void phi::ConvCudnnGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::string const&, std::vector<int, std::allocator<int> > const&, int, std::string const&, phi::DenseTensor*, phi::DenseTensor*)
5   phi::SearchResult<cudnnConvolutionBwdDataAlgo_t> phi::SearchAlgorithm<(phi::ConvKind)2>::Find<float>(phi::GPUContext const&, phi::ConvArgsBase<cudnnContext*, cudnnDataType_t> const&, bool, bool, bool)

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1749993773 (unix time) try "date -d @1749993773" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0x3e8000a8733) received by PID 690058 (TID 0x7ada799ad740) from PID 689971 ***]

